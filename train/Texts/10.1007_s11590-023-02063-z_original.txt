
Linear Convergence Rate Analysis of Proximal Generalized ADMM for Convex Composite Programming
August 19, 2022

Han Wang 
Yunhai Xiao 
Linear Convergence Rate Analysis of Proximal Generalized ADMM for Convex Composite Programming
August 19, 2022Generalized ADMMproximal termscalmnesslinear convergence
The proximal generalized alternating direction method of multipliers (p-GADMM) is substantially efficient for solving convex composite programming problems of high-dimensional to moderate accuracy. The global convergence of this method was established by Xiao, Chen & Li [Math. Program. Comput., 2018], but its convergence rate was not given. One may take it for granted that the convergence rate could be proved easily by mimicking the proximal ADMM, but we find the relaxed points will certainly cause many difficulties for theoretical analysis. In this paper, we devote to exploring its convergence behavior and show that the sequence generated by p-GADMM possesses Q-linear convergence rate under some mild conditions. We would like to note that the proximal terms at the subproblems are required to be positive definite, which is very common in most practical implementations although it seems to be a bit strong.

Introduction

Let Y := Y 1 × · · · × Y m and Z := Z 1 × · · · × Z n be finite-dimensional real Euclidean spaces, each endowed with an inner product ·, · as well as its induced norm · . In this paper, we consider a canonical class of convex composite minimization problem with a separable objective function and linear constraint min f (y) + g(z)
s.t. A * y + B * z = c,(1.1)
where f : Y → (−∞, +∞] and g : Z → (−∞, +∞] are closed proper convex but not necessarily smooth functions, A : X → Y and B : X → Z are linear operators with their adjoints A * and B * , respectively, c ∈ X is the given date. Given this structure, problem (1.1) arises in a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others [2]. To solve (1.1), a simple but powerful algorithm is the alternating direction method of multipliers (ADMM) designed originally by Glowinski & Marroco [17] and Gabay & Mercier [15], whose construction was closely correlated with Rockafellar's work on proximal point algorithm (PPA) for solving a more general maximal monotone inclusion problem [23,24]. The readers may refer to [2,16] for reviewing the historical development of ADMM. Starting from (y 0 , z 0 , x 0 ) ∈ Y × Z × X , the iterative scheme of ADMM for solving (1.1) takes the following form, for k = 0, 1, . . . ,        y k+1 = arg min y∈Y L σ (y, z k ; x k ), z k+1 = arg min z∈Z L σ (y k+1 , z; x k ),

x k+1 = x k − τ σ(A * y k+1 + B * z k+1 − c), (1.2) where τ ∈ (0, (1 + √ 5)/2) is a step-length, and L σ (y, z; x) is the augmented Lagrangian function of problem (1.1) defined as
L σ (y, z; x) := f (y) + g(z) − x, A * y + B * z − c + σ 2 A * y + B * z − c 2 ,
where x ∈ X is a multiplier and σ > 0 is a penalty parameter. It is interesting to note that the iteration scheme (1.2) is not always well-defined, one can consult [7] for a counter-example. Under the existence condition of a solution to the Karush-Kuhn-Tucker (KKT) system of (1.1), Gabay [14] showed that the ADMM (1.2) with an unit steplength is actually equivalent to the well-known Douglas-Rachford splitting (DRs) method [20] to find a zero point to the stationary system coming from the dual of (1.1). Also, DRs can be considered as an application of PPA, see [4,11]. As a result, Eckstein & Bertsekas [11] applied an accelerated technique to (1.2) so as to getting a generalized variant ADMM, that is
         y k+1 = arg min y∈Y L σ (y, z k ; x k ), z k+1 = arg min z∈Z g(z) − Bx k , z + σ 2 ρ(A * y k+1 + B * z k − c) + B * (z − z k ) 2 , x k+1 = x k − σ ρ(A * y k+1 + B * z k − c) + B * (z k+1 − z k ) ,(1.3)
where ρ ∈ (0, 2) is a relaxation factor. It is quite clear to see that the (1.2) with τ = 1 is consistent with (1.3) under the setting ρ = 1. The generalized ADMM retains the benefits of treating the objective functions f and g individually, and at the same time, it also enjoys the easiness to implement. Most importantly, a suitable ρ may lead to better numerical performance. For the empirical studies of the generalized ADMM, one can refer to [1,3,9,29].

To make the subproblems in (1.2) admit unique solutions without further assumptions on the objective functions and constraints, Eckstien [10] suggested add a proximal term to each subproblem. Later, Fazel et al. [13] showed that these added proximal terms are not necessarily positive definite, and then proposed a more powerful but convenient semi-proximal ADMM (abbr. sPADMM). The sPADMM covers the classic ADMM as a special case and has the ability to deal with multi-block convex composite semidefinite programming problems of a low to moderate accuracy [6,19,27]. In recent years, Xiao, Chen & Li [29] introduced a variant of generalized ADMM with semi-proximal terms (p-GADMM), that is, starting
fromω 0 := (x 0 ,ỹ 0 ,z 0 ) ∈ X × domf × domg, it generates a sequence {(y k , z k , x k )} using the following frameworks                y k := arg min y∈Y L σ (y,z k ;x k ) + 1 2 y −ỹ k 2 S , x k :=x k − σ(A * y k + B * zk − c), z k := arg min z∈Z L σ (y k , z; x k ) + 1 2 z −z k 2 T , ω k+1 :=ω k + ρ(ω k −ω k ). (1.4)
In fact, this variant is based on an important observation of Chen [5] that the generalized ADMM (1.3) can be reformulated as an ADMM with an extra relaxation step with factor lying in (0, 2). By comparison with [21], the semi-proximal terms in (1.4) is more natural and pretty in sense that it used the most recent values of variables. Moreover, extensive numerical experiments on a class of linearly doubly non-negative semidefinite programming problems illustrated that the variant of generalized ADMM (1.4) performed more effectively and efficiently [29].

It has been proved that the sequence generated by p-GADMM converges globally to the KKT point of (1.1) under some mild conditions. However, its convergence rate was not given. In fact, the convergence rate analysis on ADMM and its related variants have been studied by in different contexts. For example, under only an error bound condition, Han, Sun & Zhang [18] established the linear rate convergence rate of sPADMM of Fazel et al. [13] with τ ∈ 0, 1 + √ 5 /2 . For another example, Fang et al. [12] derived the linear convergence rate of a linearized variant of generalized ADMM and proved the worst-case O (1/k) iteration complexity in both ergodic and nonergotic cases. This result was further improved by Wang et al. [28], in which they showed that the Q-linear convergence result for generalized ADMM (1.3) hold if the proximal terms are positive and semidefinite. But despite these achievements, the convergence rate of p-GADMM with respect to (1.4) is not trivial because the relaxed iterative points (x k ,ỹ k ,z k ) would certainly cause many difficulties. The paper concentrates on theoretical analysis to prove that the sequence {(y k , z k )} generated by p-GADMM possesses Q-linear convergence rate under the condition of calmness.

The remaining parts of this paper are organized as follows. Section 2 is divided into two subsections, Subsection 2.1 presents some results on the optimimality conditions for problem (1.1), and Subsection 2.2 briefly overviews the definitions and properties associated with calmness in variational analysis. Section 3 is the main part of this paper, in which, we derive the local linear convergence results for p-GADMM under some certain assumption conditions. Finally, we conclude the paper in Section 4.


Preliminaries

The section presents some notations and basic concepts appeared in the context, and summarizes some useful preliminaries used for later analysis.


Notations and basic concepts

For any two vectors x ∈ R n and y ∈ R m , we use (x, y) to denote their adjunction, i.e., (x, y) ∈ R m+n . For p ≥ 1, we use x p to denote an p -norm of a vector x, and for p = 2, we simply denote it as x . For any symmetric and positive definite matrix O, the O-norm of x is denoted by
x O := √
x Ox. For a given closed convex set C, the distance of
x to C regarding O-norm is denoted as dist O (x, C) := inf y∈C x − y O . Give f : Y → (−∞, +∞] be a proper closed convex function. We use domf to denote the domain of f , that is, domf = {y ∈ Y | f (y) < ∞}. The proximal mapping of f with t > 0 is defined by Prox f (x) := argmin y∈Y f (y) + 1 2 y − x 2 2 , ∀x ∈ Y.
The Lagrangian function of problem (1.1) is defined by
L(y, z; x) := f (y) + g(z) − x, A * y + B * z − c , ∀(y, z, x) ∈ Y × Z × X , (2.1)
which is convex in (y, z) ∈ Y × Z and concave in x ∈ X . The Slater constraint qualification for problem (1.1) is said to be held if
(y, z) | y ∈ ri(domf ), z ∈ ri(domg), A * y + B * z = c = ∅,
where ri(·) denotes the relative interior of a convex set. Under this constraint qualification, from [25, Corollaries 28.2.2 and 28.3.1], we know that (ȳ,z) ∈ ri(domf × domg) is an optimal solution to problem (1.1) if and only if there exists a Lagrange multiplierx ∈ X such that (ȳ,z,x) is a solution to the following KKT system:
Ax ∈ ∂f (y), Bx ∈ ∂g(z) and A * y + B * z − c = 0, (2.2)
where ∂f and ∂g are the subdifferential mappings of f and g in (1.1), respectively. Noting that the subdifferential mappings ∂f and ∂g are maximal monotone [26,Theorem 12.17], then for all y, y ∈ domf , ξ ∈ ∂f (y) and ξ ∈ ∂f (y ), it holds
ξ − ξ, y − y ≥ y − y 2 Σ f ,
and for all z, z ∈ domg, ζ ∈ ∂g(z) and ζ ∈ ∂g(z ), it holds
ζ − ζ, z − z ≥ z − z 2 Σg ,
where Σ f : Y → Y and Σ g : Z → Z are self-adjoint and positive semidefinite linear operators. LetΩ := (ȳ,z,x) be the solution set to the KKT system (2.2). The nonempty ofΩ can be guaranteed under the setting of a certain constraint qualification condition. In this paper, we only assume the existence of KKT point, and do not particularly emphasize which qualification is used.

Assumption 2.1. The KKT system (2.1) has a nonempty solution set, i.e.,Ω = ∅.

In order to facilitate the subsequent theoretical analysis, we let ν :
= (x, y,ỹ − y, z,z − z) ∈ V := X × Y × Y × Z × Z for givenỹ ∈ Y andz ∈ Z, and define a KKT mappingR : V → V in the form of R(ν) :=        A * y + B * z − c y − Prox f (y + Ax) 0 z − Prox g (z + Bx) 0        , ∀ν ∈ V, (2.3)
where Prox f (·) represents the proximal mapping of a closed convex proper function f . Defineν := (x,ȳ, 0,z, 0) such that (x,ȳ,z) ∈Ω. For simplicity, we denote a generalized optimal solutions setΘ := {(0, 0)} ∪Ω, which means that (x,ȳ,z) ∈Ω if and only ifν ∈Θ. By optimization theory, we know that the proximal mappings Prox f (·) and Prox g (·) are globally Lipschitz continuous with modulus one. Then, the mappingR(·) is continuous on V andR(ν) = 0 if and only ifν ∈Θ.


Locally upper Lipschitzian and calmness

Given a set-valued function, say Ψ, from X to Y, the graph of Ψ is defined as
Γ Ψ := {(x, y) ∈ X × Y | y ∈ Ψ(x)}.
For convenience, we denote B Y as an Euclidean unit ball, i.e., B Y := {y ∈ Y| y ≤ 1}.
Definition 2.1 ([22]
). The set-valued mapping Ψ : X ⇒ Y is said to be locally upper Lipschitzian at a point x 0 ∈ X with modulus λ, if for some neighborhoods N of x 0 and for all x ∈ N such that
Ψ(x) ⊆ Ψ(x 0 ) + λ x − x 0 B Y , ∀x ∈ N .
The set-valued mapping Ψ is called piecewise polyhedral if its graph Γ Ψ is the union of finitely many polyhedral sets. The elementary relationship between the locally upper Lipschitzian and the piecewise polyhedral for a set-valued mapping Ψ is stated as follows: 22]). Let set-valued mapping Ψ be piecewise polyhedral from X into Y, then there exists a constant λ such that Ψ is locally upper Lipschitzian at each x 0 ∈ X independent of the choice of x 0 .
Proposition 2.1 ([
It was known from [26, definition 10.20] that, if a set-valued function Ψ is called piecewise linearquadratic, then domΨ can be represented as the union of finitely many polyhedral sets, relative to each of which Ψ is either given by an expression of affine or quadratic function. Meanwhile, Ψ is piecewise linear-quadratic if and only if the subdifferential mapping ∂Ψ is piecewise polyhedral. The proof and its extensions can be found at the monograph [26, proposition 12.30].

We now ready to state the definition of calmness for Ψ : X ⇒ Y at x 0 for y 0 with (x 0 , y 0 ) ∈ Γ Ψ . For more details, one can see the disquisition of Dontchev & Rockafellar [8] and Rockafellar & Wets [26].
Definition 2.2 ([8])
. A set-valued mapping Ψ : X ⇒ Y is called to be calm at x 0 for y 0 if (x 0 , y 0 ) ∈ Γ Ψ and there exists a constant λ along with neighborhoods N of x 0 and M of y 0 such that
Ψ(x) ∩ M ⊆ Ψ(x 0 ) + λ x − x 0 B Y , ∀x ∈ N .
As can be seen from this definition that, suppose Ψ be the subdifferential mapping of a piecewise linearquadratic function, then Ψ is calm at x 0 for y 0 meeting (x 0 , y 0 ) ∈ Γ Ψ with modulus λ ≥ 0 independent of the selection of (x 0 , y 0 ). At last, a set-valued mapping Ψ : X ⇒ Y is called metrically subregular at x 0 for y 0 if (x 0 , y 0 ) ∈ Γ Ψ and there exists a constant ι ≥ 0 along with neighborhoods N of x 0 and M of
y 0 such that dist(x, Ψ −1 (y 0 )) ≤ ι dist(y 0 , Ψ(x) ∩ M), ∀x ∈ N ,
which is a.k.s. error bound condition. To end this section, we list the following result to reveal the equivalence of metric subregularity of a set-valued mapping with calmness of its inverse. For its proof, one can refer to [8,Theorem 3H.3].
Proposition 2.2 ([8])
. For a set-valued mapping Ψ : X ⇒ Y, let (x 0 , y 0 ) ∈ Γ Ψ . Then Ψ is metrically subregular at x 0 for y 0 with a constant λ if and only if its inverse Ψ −1 : Y ⇒ X is calm at y 0 for x 0 with the same constant λ.


Linear convergence rate

In this section, we present a general convergence rate analysis on algorithm p-GADMM. It should be noted that the steps of p-GADMM has been fully stated in [29], but for the convenience of the subsequent analysis, we adjust the updating order and the upper script here. From [29, Lemma 5.2, Theorem 5.1], it is a trivial task to get the following result which provides some useful highlights for the further convergence rate analysis.

Theorem 3.1. Suppose that the KKT system (2.2) is nonempty. Let the sequence {(x k , y k , z k ;x k ,ỹ k ,z k )} be generated by Algorithm 1. Then the following results hold: 
(i) For any k ≥ 0, (σρ) −1 x k e + σ(1 − ρ)A * y k e 2 + ρ −1 ỹ k+1 e 2 S + ρ −1 z k+1 e 2 T + (2 − ρ) y k −ỹ k 2 S + (2 − ρ) σ A * y k e 2 ≥(σρ) −1 x k+1 e + σ(1 − ρ)A * y k+1+ (2 − ρ) ỹ k+1 − y k+1 2 S + (2 − ρ) z k+1 − z k+1 2 T + σρ −1 (2 − ρ) 2 A * (y k+1 − y k ) 2 ,(3.

2)


Algorithm 1 p-GADMM

Step 0. Let σ ∈ (0, +∞) and ρ ∈ (0, 2) be given parameters. Let S and T be self-adjoint positive definite linear operators defined on Y and Z, respectively. Choose (x 0 ,ỹ 0 ,z 1 ) ∈ X × domf × domg.

Step 1. Compute    y 0 := arg min y∈Y L σ (y,z 1 ;x 0 ) + 1 2 y −ỹ 0 2 S ,
x 0 :=x 0 − σ(A * y 0 + B * z1 − c).
Step 2. For k = 1, 2, 3, . . ., do the following steps iteratively:
                             z k := arg min z∈Z L σ (y k−1 , z; x k−1 ) + 1 2 z −z k 2 T , (3.1a) y k :=ỹ k−1 + ρ(y k−1 −ỹ k−1 ), (3.1b) x k :=x k−1 + ρ(x k−1 −x k−1 ), (3.1c) z k+1 :=z k + ρ(z k −z k ), (3.1d) y k := arg min y∈Y L σ (y,z k+1 ;x k ) + 1 2 y −ỹ k 2 S , (3.1e) x k :=x k − σ(A * y k + B * zk+1 − c). (3.1f)
Step 3. If a termination criterion is not met, set k := k + 1 and go to Step 2.

where x e = x −x, y e = y −ȳ, and z e = z −z.

(ii) Assume that both S and T be chosen such that Σ f + S + σAA * 0 and Σ g + T + σBB * 0, then the sequence (x k , y k ,ỹ k − y k , z k ,z k − z k ) is automatically well-defined, and it converges to (x,ȳ, 0,z, 0) ∈Θ. Theorem 3.1 presents a global convergence result for p-GADMM under fairly general and mild conditions. Evidently, one can choose positive semidefinite (and even indefinite) linear operators S and T to ensure Σ f + S + σAA * 0 and Σ g + T + σBB * 0. But, due to the existences of some coupling terms in (3.2), we must restrict S and T to be positive definite. In this case, the conditions Σ f + S + σAA * 0 and Σ g + T + σBB * 0 hold automatically.

We now present some notations to facilitate the later theoretical analysis. For any self-adjoint linear operator G : X → X , we use the symbol λ max (G) to denote its largest eigen-value. Denote
Υ(ν k+1 ) := κ ỹ k+1 − y k+1 2 S + z k+1 − z k+1 2 T + A * y k+1 e + B * z k+1 e 2 + A * (y k+1 − y k ) 2 , (3.3) where κ := max S , 3 T , 3(2 − ρ) 2 σ 2 λ max (B * B), 3(1 − ρ) 2 σ 2 λ max (B * B) + 1 . (3.4)
Moreover, denote Ξ : V → V in the form of
Ξ :=     (σρ) −1 I (1−ρ)ρ −1 A * 0 0 0 (1−ρ)ρ −1 A ρ −1 σAA * +ρ −1 S+2Σ f (1−ρ)ρ −1 S 0 0 0 (1−ρ)ρ −1 S ρ −1 S 0 0 0 0 0 ρ −1 T +2Σg (1−ρ)ρ −1 T 0 0 0 (1−ρ)ρ −1 T (1−ρ) 2 ρ −1 T     + 1 2 (2 − ρ)σϑϑ * ,
where I is an identity operator, and ϑ : X → V is a linear operator such that its adjoint ϑ * satisfies ϑ * (ν) = A * y + B * z for any ν ∈ V. In the subsequent analysis, we use the operator Ξ to measure the weighted distance from the current point to the generalized optimal solutions setΘ. Obviously, if ρ ∈ (0, 2) and S 0, T 0, then Ξ must be positive definite, i.e.,
{S 0 & T 0} ⇔ Ξ 0,
which plays a key rule in the linear convergence rate result. Additionally, to conduct the rate of the decrease of ν k −ν 2 , we take an interest in deducing an upper bound forR(·) computed at the sequence generated by the p-GADMM in the subsequent developments.

Lemma 3.1. Let ν k be the infinite sequence generated by p-GADMM. Then for any k ≥ 1, we have
Υ(ν k+1 ) ≥ R (ν k+1 ) 2 . (3.5)
Proof. From (3.1c) and (3.1f), it is easy to see that
x k+1 = x k + (1 − ρ)(x k − x k ) = x k + σ(1 − ρ)(A * y k e + B * zk+1 e ).
Then, substituting this equality into (3.1f), we get
x k+1 =x k+1 − σ(A * y k+1 e + B * zk+2 e ) = x k − σρ(A * y k+1 e + B * z k+1 e ) + σ(1 − ρ)A * (y k − y k+1 ).
(3.6)

By the first order optimality condition of (3.1a), we have
0 ∈ ∂g(z k+1 ) − Bx k + σB(A * y k e + B * z k+1 e ) + T (z k+1 −z k+1 ) = ∂g(z k+1 ) − B x k − σ(A * y k+1 e + B * z k+1 e ) + σA * (y k+1 − y k ) + T (z k+1 −z k+1 ),
which leads to an equivalent expression for z k+1 , that is,
z k+1 = Prox g (z k+1 + B x k − σ(A * y k+1 e + B * z k+1 e ) + σA * (y k+1 − y k ) − T (z k+1 −z k+1 )). (3.7)
It follows from (3.1e) we can easily get
0 ∈ ∂f (y k+1 ) − Ax k+1 + σA(A * y k+1 e + B * zk+2 e ) + S(y k+1 −ỹ k+1 ),
which, from (3.1f), is equivalent to
0 ∈ ∂f (y k+1 ) − Ax k+1 + S(y k+1 −ỹ k+1 ).
Thus, we obtain that y k+1 = Prox f y k+1 + Ax k+1 − S(y k+1 −ỹ k+1 ) . 
R (ν k+1 ) 2 ≤ A * y k+1 e + B * z k+1 e 2 + S(y k+1 −ỹ k+1 ) 2 + B(x k − x k+1 ) − σB(A * y k+1 e + B * z k+1 e ) + σBA * (y k+1 − y k ) − T (z k+1 −z k+1 ) 2 = A * y k+1 e + B * z k+1 e 2 + S(y k+1 −ỹ k+1 ) 2 + (ρ − 1)σB(A * y k+1 e + B * z k+1 e ) + (2 − ρ)σBA * (y k+1 − y k ) − T (z k+1 −z k+1 ) 2 ≤ A * y k+1 e + B * z k+1 e 2 + S y k+1 −ỹ k+1 2 S + 3 T z k+1 −z k+1 2 T + 3(1 − ρ) 2 σ 2 λ max (B * B) A * y k+1 e + B * z k+1 e 2 + 3(2 − ρ) 2 σ 2 λ max (B * B) A * (y k+1 − y k ) 2 ≤κ( A * y k+1 e + B * z k+1 e 2 + y k+1 −ỹ k+1 2 S + z k+1 −z k+1 2 T + A * (y k+1 − y k ) 2 ).
Using the definition of Υ(·) in (3.3), we get the inequality (3.5).

To get the local linear convergence rate of p-GADMM 1, we need another assumption to control the distance from an iterate ν to the KKT solution setΘ. 
dist(ν,Θ) ≤ λ R (ν) , ∀ν ∈ ν ∈ V| ν −ν ≤ ε .
In light of above analysis, we are ready to establish the local linear convergence rate result of algorithm p-GADMM.

Theorem 3.2. Suppose that Assumptions 2.1 and 3.1 hold. Besides, assume that S and T are positive definite. Let the sequence {(x k , y k , z k ;x k ,ỹ k ,z k )} be generated by Algorithm p-GADMM. Then {ν k := (x k , y k ,ỹ k − y k , z k ,z k − z k )} converges toν = (x,ȳ, 0,z, 0) ∈Θ, and there exists a thresholdκ ≥ 1 such that for all k ≥κ, it holds that dist 2 Ξ ν k+1 ,Θ ≤ αdist 2 Ξ ν k ,Θ , (3.9) where α := (1 + β) −1 and β := (2 − ρ) min 1,
1 2 σ, σρ −1 (2 − ρ) λ 2 κλ max (Ξ) −1 ,
where κ is defined in (3.4). Moreover, there exists a positive number ζ ∈ [α, 1) such that for all k ≥κ
dist 2 Ξ ν k+1 ,Θ ≤ ζdist 2 Ξ ν k ,Θ . (3.10)
Proof. From the part (i) of Theorem 3.1, it holds that
(σρ) −1 x k e + σ(1 − ρ)A * y k e 2 + ρ −1 ỹ k+1 e 2 S + ρ −1 z e k+1 2 T + (2 − ρ) σ A * y k e 2 + (2 − ρ) y k −ỹ k 2 S + 1 2 (2 − ρ)σ ϑ * (ν k e ) 2 + 2 y k e 2 Σ f + 2 z k e 2 Σg ≥(σρ) −1 x k+1 e + σ(1 − ρ)A * y k+1 e 2 + ρ −1 ỹ k+2 e 2 S + ρ −1 z e k+2 2 T + (2 − ρ) y k+1 −ỹ k+1 2 S + (2 − ρ) σ A * y k+1 e 2 + 2 y k+1 e 2 Σ f + 2 z k+1 e 2 Σg + 1 2 (2 − ρ)σ ϑ * (ν k+1 e ) 2 + (2 − ρ) ỹ k+1 − y k+1 2 S + (2 − ρ) z k+1 − z k+1 2 T + σρ −1 (2 − ρ) 2 A * (y k+1 − y k ) 2 + 1 2 (2 − ρ)σ A * y k+1 e + B * z k+1 e 2 ,
which implies that
ν k e 2 Ξ ≥ ν k+1 e 2 Ξ + (2 − ρ) ỹ k+1 − y k+1 2 S + z k+1 − z k+1 2 T +σρ −1 (2 − ρ) A * (y k+1 − y k ) 2 + 1 2 σ A * y k+1 e + B * z k+1 e 2 .
(3.11)

BecauseΘ is a nonempty closed convex set, we can immediately get (3.11) to the following required result
dist 2 Ξ (ν k ,Θ) ≥ dist 2 Ξ (ν k+1 ,Θ) + (2 − ρ) ỹ k+1 − y k+1 2 S + z k+1 − z k+1 2 T +σρ −1 (2 − ρ) A * (y k+1 − y k ) 2 + 1 2 σ A * y k+1 e + B * z k+1 e 2 .
(3.12)

Observing the structure of right hand side of (3.12) and the definition of Υ(ν k+1 ) in (3.3), we know for all k ≥ 1 and ρ ∈ (0, 2) that
κ ỹ k+1 − y k+1 2 S + z k+1 − z k+1 2 T + σρ −1 (2 − ρ) A * (y k+1 − y k ) 2 + 1 2 σ A * y k+1 e + B * z k+1 e 2 ≥ min 1, 1 2 σ, σρ −1 (2 − ρ) Υ(ν k+1 ).
(3.13)

According to part (ii) of Theorem 3.1, we know that the sequence (x k , y k ,ỹ k − y k , z k ,z k − z k ) converges toν k = (x,ȳ, 0,z, 0), which means that there existsκ ≥ 1 and ε > 0 such that for any k ≥κ, it holds that ν k+1 −ν ≤ ε.

Subsequently, from Assumption 3.1 and Lemma 3.1, it gets for all k ≥κ that
dist 2 (ν k+1 ,Θ) ≤ λ 2 R (ν k+1 ) 2 ≤ λ 2 Υ(ν k+1 ).
(3.14)

Combining (3.13) with (3.14) and using the fact ρ ∈ (0, 2), it yields that
(2 − ρ) ỹ k+1 − y k+1 2 S + z k+1 − z k+1 2 T +σρ −1 (2 − ρ) A * (y k+1 − y k ) 2 + 1 2 σ A * y k+1 e + B * z k+1 e 2 ≥(2 − ρ) min 1, 1 2 σ, σρ −1 (2 − ρ) λ −2 κ −1 dist 2 (ν k+1 ,Θ)
≥β dist 2 Ξ (ν k+1 ,Θ).

(3.15) From (3.15), we can derive the assertion (3.9). Recalling that α < 1, this readily ensures the fulfillment of linear convergence rate (3.10).

From Theorem 3.2, we know that the conclusion of local linear convergence rate for Algorithm p-GADMM relies on the calmness property. As discussed in section 2.2 that, for any set-valued mapping Ψ, the property of calmness at a certain point holds automatically if Ψ is piecewise polyhedral, and particularly, Ψ is a subdifferential mapping of a convex piecewise linear-quadratic function. Based on the fact that Ψ −1 is piecewise polyhedral if and only if Ψ itself is piecewise polyhedral, we can get that when piecewise polyhedral condition is imposed on the mappingR(·), then the global linear convergence rate of Algorithm p-GADMM can be derived.

Corollary 3.1. Let the sequence {(x k , y k , z k ;x k ,ỹ k ,z k )} be generated by Algorithm p-GADMM. Let ν k := (x k , y k ,ỹ k − y k , z k ,z k − z k ) and ρ ∈ (0, 2), and letν = (x,ȳ, 0,z, 0) ∈Θ be any limiting point of ν k . Suppose that the solution set to the KKT system (2.2) is nonempty and that S and T are positive definite. Besides, suppose that the mappingR(·) is piecewise polyhedral. Then, there exists a constant λ > 0 such that for all k ≥ 1 , dist ν k ,Θ ≤λ R ν k , (3.16) and dist 2 Ξ ν k+1 ,Θ ≤αdist 2 Ξ ν k ,Θ , Proof. On the one hand, from the nonempty setΘ and the piecewise polyhedral condition, there exist fixed λ > 0 and δ > 0 such that dist(ν k ,Θ) ≤ λ R (ν k )

if R (ν k ) ≤ δ . On the other hand, following the proof of Theorem 3.2, for all k ≥ 1, we know that {ν k := (x k , y k ,ỹ k − y k , z k ,z k − z k )} converges toν = (x,ȳ, 0,z, 0) while ν k −ν ≤ ε with constant ε > 0. For the ν k satisfying R (ν k ) > δ, we get dist(ν k ,Θ) ≤ ν k −ν ≤ ε < εδ −1 R (ν k ) .

We readily obtain that there exists a positive numberλ := max λ, εδ −1 such that (3.16) holds. Employing a similar proof of Theorem 3.2 yields the inequality (3.17), thereby the global linear convergence rate holds.

At the end of this section, we notice that the assumption onR implies that the condition dist ν k ,Θ ≤ λ R ν k hold automatically. Therefore, the global linear convergence rate can be achieved.


Conclusion

We know that the method of p-GADMM proposed by Xiao, Chen & Li [29] is highly efficient for convex composite programming problems. The global convergence of p-GADMM is known, but its convergence rate is worthy of exploring. This paper was devoting to provide a theoretical analysis and proved that p-GADMM has Q-linear convergence rate under the assumption that the KKP mapping is calm. This conclusion is consistent with the theoretical result of Han et al. [18] to the semi-proximal ADMM of Fazel et al. [13]. Nevertheless, it is still worth emphasizing that Theorem 3.2 requires S and T being positive definite, which is slightly stronger than the one in [18]. Despite this, we believe that this condition will not affect the contribution of this paper because it actually common in vast majority of practical implementations.


Acknowledgments

The work of Y. Xiao is supported by the National Natural Science Foundation of China (Grant No. 11971149).


2 − ρ) y k+1 −ỹ k+1 2 S + (2 − ρ) σ A * y + (2 − ρ)σ A * y k+1 e + B * z k+1 e 2

Assumption 3 . 1 .
31The inverse of the mappingR(·) defined in (2.3) is calm at the 0 ∈ V forν with modulus λ > 0 if there exists a constant ε > 0 such that

Constrained optimization and lagrange multiplier methods. D P Bertsekas, Computer Science and Applied Mathematics. D.P. Bertsekas. Constrained optimization and lagrange multiplier methods. Computer Science and Applied Mathematics, 1982.

Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning. S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, 3S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3:1-122, 2011.

A proximal point algorithm revisit on the alternating direction method of multipliers. X Cai, G Gu, B S He, X Yuan, Science China Mathematics. 56X. Cai, G. Gu, B.S. He, and X.M Yuan. A proximal point algorithm revisit on the alternating direction method of multipliers. Science China Mathematics, 56:2179-2186, 2013.

The developments of proximal point algorithms. X.-J Cai, K Guo, F Jiang, K Wang, Z.-M Wu, D.-R Han, Journal of the Operations Research Society of China. X.-J. Cai, K. Guo, F. Jiang, K. Wang, Z.-M. Wu, and D.-R. Han. The developments of proximal point algorithms. Journal of the Operations Research Society of China, pages 1-43, 2022.

Numerical algorithms for a class of matrix norm approximation problems. C H Chen, C.H. Chen. Numerical algorithms for a class of matrix norm approximation problems. 2012.

An efficient inexact symmetric gauss-seidel based majorized admm for high-dimensional convex composite conic programming. L Chen, D F Sun, K.-C Toh, Mathematical Programming. 1611L. Chen, D.F. Sun, and K.-C. Toh. An efficient inexact symmetric gauss-seidel based majorized admm for high-dimensional convex composite conic programming. Mathematical Programming, 161(1):237-270, 2017.

A note on the convergence of admm for linearly constrained convex optimization problems. L Chen, D F Sun, K.-C Toh, Computational Optimization and Applications. 662L. Chen, D.F. Sun, and K.-C. Toh. A note on the convergence of admm for linearly constrained convex optimization problems. Computational Optimization and Applications, 66(2):327-343, 2017.

Implicit functions and solution mappings. A L Dontchev, R T Rockafellar, SpringerA.L. Dontchev and R.T. Rockafellar. Implicit functions and solution mappings. Springer, 2009.

Parallel alternating direction multiplier decomposition of convex programs. J Eckstein, Journal of Optimization Theory and Applications. 80J. Eckstein. Parallel alternating direction multiplier decomposition of convex programs. Journal of Optimization Theory and Applications, 80:39-62, 1994.

Some saddle-function splitting methods for convex programming. J Eckstein, Optimization Methods & Software. 4J. Eckstein. Some saddle-function splitting methods for convex programming. Optimization Methods & Software, 4:75-83, 1994.

On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators. J Eckstein, D P Bertsekas, Mathematical Programming. 55J. Eckstein and D.P. Bertsekas. On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Programming, 55:293-318, 1992.

Generalized alternating direction method of multipliers: new theoretical insights and applications. E X Fang, B S He, H Liu, X M Yuan, Mathematical Programming Computation. 72E.X. Fang, B.S. He, H. Liu, and X.M. Yuan. Generalized alternating direction method of multipliers: new theoretical insights and applications. Mathematical Programming Computation, 7(2):149-187, 2015.

Hankel matrix rank minimization with applications to system identification and realization. M Fazel, T K Pong, D F Sun, P Tseng, SIAM Journal on Matrix Analysis and Applications. 34M. Fazel, T.K. Pong, Sun D.F., and P. Tseng. Hankel matrix rank minimization with applications to system identification and realization. SIAM Journal on Matrix Analysis and Applications, 34:946- 977, 2013.

Applications of the method of multipliers to variational inequalities. D Gabay, Studies in mathematics and its applications. Elsevier15D. Gabay. Applications of the method of multipliers to variational inequalities. In Studies in mathematics and its applications, volume 15, pages 299-331. Elsevier, 1983.

A dual algorithm for the solution of nonlinear variational problems via finite element approximation. D Gabay, B Mercier, Computers & Mathematics With Applications. 2D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers & Mathematics With Applications, 2:17-40, 1976.

On alternating direction methods of multipliers: A historical perspective. R Glowinski, Modeling, Simulation and Optimization for Science and Technology. R. Glowinski. On alternating direction methods of multipliers: A historical perspective. In Modeling, Simulation and Optimization for Science and Technology, 2014.

Sur l'approximation, paréléments finis d'ordre un, et la résolution, par pénalisation-dualité d'une classe de problèmes de dirichlet non linéaires. R Glowinski, A Marroco, ESAIM: Mathematical Modelling and Numerical Analysis-Modélisation Mathématique et Analyse Numérique. 9R2R. Glowinski and A. Marroco. Sur l'approximation, paréléments finis d'ordre un, et la résolution, par pénalisation-dualité d'une classe de problèmes de dirichlet non linéaires. ESAIM: Mathematical Modelling and Numerical Analysis-Modélisation Mathématique et Analyse Numérique, 9(R2):41-76, 1975.

Linear rate convergence of the alternating direction method of multipliers for convex composite programming. D R Han, D F Sun, L W Zhang, Mathematics of Operations Research. 43D.R. Han, D.F. Sun, and L.W. Zhang. Linear rate convergence of the alternating direction method of multipliers for convex composite programming. Mathematics of Operations Research, 43:622-637, 2018.

A two-phase augmented Lagrangian method for convex composite quadratic programming. X D Li, Department of Mathematics, National University of SingaporePhD thesisX.D. Li. A two-phase augmented Lagrangian method for convex composite quadratic programming. PhD thesis, PhD thesis, Department of Mathematics, National University of Singapore, 2015.

Splitting algorithms for the sum of two nonlinear operators. P L Lions, B Mercier, SIAM Journal on Numerical Analysis. 16P.L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis, 16:964-979, 1979.

Convergence analysis on a modified generalized alternating direction method of multipliers. S Lu, Z Wei, Journal of Inequalities and Applications. 20181S. Lu and Z. Wei. Convergence analysis on a modified generalized alternating direction method of multipliers. Journal of Inequalities and Applications, 2018(1):1-14, 2018.

Some continuity properties of polyhedral multifunctions. S M Robinson, Mathematical Programming at Oberwolfach. SpringerS.M. Robinson. Some continuity properties of polyhedral multifunctions. In Mathematical Program- ming at Oberwolfach, pages 206-214. Springer, 1981.

Augmented lagrangians and applications of the proximal point algorithm in convex programming. R T Rockafellar, Mathematics of Operations Research. 1R.T. Rockafellar. Augmented lagrangians and applications of the proximal point algorithm in convex programming. Mathematics of Operations Research, 1:97-116, 1976.

Monotone operators and the proximal point algorithm. R T Rockafellar, SIAM Journal on Control and Optimization. 14R.T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14:877-898, 1976.

R T Rockafellar, Convex Analysis. PrincetonPrinceton University PressR.T. Rockafellar. Convex Analysis. Princeton University Press, Princeton(1970).

R T Rockafellar, R , J-B Wets, Variational Analysis. springerR.T. Rockafellar and R.J-B. Wets. Variational Analysis. springer(1998).

A convergent 3-block semiproximal alternating direction method of multipliers for conic programming with 4-type constraints. D F Sun, K.-C Toh, L Q Yang, SIAM Journal on Optimization. 252D.F. Sun, K.-C. Toh, and L.Q. Yang. A convergent 3-block semiproximal alternating direction method of multipliers for conic programming with 4-type constraints. SIAM Journal on Optimiza- tion, 25(2):882-915, 2015.

On the linear convergence rate of generalized admm for convex composite programming. H Wang, P L Li, Y H Xiao, arXiv:2206.03649H. Wang, P.L. Li, and Y.H. Xiao. On the linear convergence rate of generalized admm for convex composite programming. arXiv:2206.03649, 2022.

A generalized alternating direction method of multipliers with semi-proximal terms for convex composite conic programming. Y H Xiao, L Chen, D H Li, Mathematical Programming Computation. 104Y. H. Xiao, L. Chen, and D. H. Li. A generalized alternating direction method of multipliers with semi-proximal terms for convex composite conic programming. Mathematical Programming Computation, 10(4):533-555, 2018.
