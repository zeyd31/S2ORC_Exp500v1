
Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach
February 22, 2022

Haotian Gu 
Xin Guo 
Xiaoli Wei 
Renyuan Xu 
Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach
February 22, 2022Multi-Agent Reinforcement LearningMean-field Cooperative GamesNeural Network Approximation
One of the challenges for multi-agent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. While exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little is known theoretically for decentralized MARL with the network of states for modeling self-driving vehicles, ride-sharing, and data and traffic routing.This paper proposes a framework of localized training and decentralized execution to study MARL with network of states. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that agents can execute afterwards the learned decentralized policies, which depend only on agents' current states.The theoretical analysis consists of three key components: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents, enabling updating the associated team Q-function in a localized fashion; the second is the Bellman equation for the value function and the appropriate Q-function on the probability measure space; and the third is the exponential decay property of the team Q-function, facilitating its approximation with efficient sample efficiency and controllable error.The theoretical analysis paves the way for a new algorithm LTDE-Neural-AC, where the actor-critic approach with over-parameterized neural networks is proposed. The convergence and sample complexity is established and shown to be scalable with respect to the sizes of both agents and states. To the best of our knowledge, this is the first neural network based MARL algorithm with network structure and provably convergence guarantee.

Introduction

Multi-agent reinforcement learning (MARL) has achieved substantial successes in a broad range of cooperative games and their applications, including coordination of robot swarms (Hüttenrauch et al. [28]), self-driving vehicles (Shalev-Shwartz et al. [48], Cabannes et al. [5]), real-time bidding games (Jin et al. [31]), ride-sharing (Li et al. [35]), power management (Zhou et al. [66]) and traffic routing (El-Tantawy et al. [16]). One of the challenges for the development of MARL is designing efficient learning algorithms for a large system, in which each individual agent has only limited or partial information of the entire system. In such a system, it is necessary to design algorithms to learn policies of the decentralized type, i.e., policies that depend only on the local information of each agent.

In a simulated or laboratory setting, decentralized policies may be learned in a centralized fashion. It is to train a central controller to dictate the actions of all agents. Such paradigm of centralized training with decentralized execution has achieved significant empirical successes, especially with the computational power of deep neural networks (Lowe et al. [40], Foerster et al. [17], Chen et al. [14], Rashid et al. [47], Yang et al. [57], Vadori et al. [52]). Such a training approach, however, suffers from the curse of dimensionality as the computational complexity grows exponentially with the number of agents (Zhang et al. [62]); it also requires extensive and costly communications between the central controller and all agents (Rabbat and Nowak [45]). Moreover, policies derived from the centralized training stage may not be robust in the execution phase (Zhang et al. [60]). Most importantly, this approach has not been supported or analyzed theoretically.

An alternative and promising paradigm is to take into consideration the network structure of the system to train decentralized policies. Compared with the centralized training approach, exploiting network structures makes the training procedure more efficient as it allows the algorithm to be updated with parallel computing and reduces communication cost.

There are two distinct types of network structures. The first is the network of agents, often found in social networks such as Facebook and Twitter, as well as team video games including StarCraft II. This network describes interactions and relations among heterogeneous agents. For MARL systems with such network of agents, Zhang et al. [63] establishes the asymptotic convergence of decentralized-actor-critic algorithms which are scalable in agent actions. Similar ideas are extended to the continuous space where deterministic policy gradient method (DPG) is used (Zhang et al. [61]), with finite-sample analysis for such framework established in the batch setting (Zhang et al. [64]). Qu et al. [44] studies a network of agents where state and action interact in a local manner; by exploiting the network structure and the exponential decay property of the Q-function, it proposes an actor-critic framework scalable in both actions and states. Similar framework is considered for the linear quadratic case with local policy gradients conducted with zero order optimization and parallel updating (Li et al. [36]).

The second type of network, the network of states, has been frequently used for modeling self-driving vehicles, ride-sharing, and data and traffic routing. It focuses on the state of agents. Compared with the network of agents which is static from agent's perspective (Sunehag et al. [50]), the network of states is stochastic: neighboring agents of any given agent may change dynamically. This type of network has been empirically studied in various applications, including packet routing (You et al. [58]), traffic routing (Calderone and Sastry [7], Guériau and Dusparic [25]), resource allocations (Cao et al. [8]) and social economic systems (Zheng et al. [65]). However, there is no existing theoretical analysis for this type of decentralized MARL. Moreover, the dynamic nature of agents' relationship makes it difficult to adopt existing methodology from the static network of agents. The goal of this paper is, therefore, to fill this void.

Our work. This paper proposes and studies multi-agent systems with network structure of agent states. In this network, homogeneous agents can move from one state to any connecting state, and observe (realistically) only partial information of the entire system in an aggregated fashion. To study this system, we propose a framework of localized training and decentralized execution (LTDE). Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that, agents can execute afterwards the learned decentralized policies which only require knowledge of agents' current states.

The theoretical analysis consists of three key components. The first is to regroup these homogeneous agents according to their states and reformulate the MARL system as a networked Markov decision process with teams of agents. This reformulation leads to the decomposition of the Q-function and the value function according to the states, enabling the update of the consequent team Q-function in a localized fashion. The second is to establish the Bellman equation for the value function and the appropriate Q-function on the probability measure space, by utilizing the homogeneity of agents. These functions are invariant with respect to the number of agents. The third is to explore the exponential decay property of the team Qfunction, enabling its approximation with a truncated version of a much smaller dimension and yet with a controllable approximation error.

To design an efficient and scalable reinforcement learning algorithm for such framework, the actor-critic approach with over-parameterized neural networks is adopted. The neural networks, representing decentralized policies and localized Q-functions, are much smaller compared with the global one. The convergence and the sample complexity of the proposed algorithm is established and shown to be scalable with respect to the size of both agents and states. To the best of our knowledge, this is the first neural network based MARL algorithm with network structure and provably convergence guarantee.

Our contribution. Our work contributes to two lines of research.

The first one is for mean-field control with reinforcement learning, for which existing works require that each agent have the full information of the population distribution (Gu et al. [24], Carmona et al. [10,11], Motte and Pham [42]) and yet in most applications agents only have access to partial or limited information (Yang et al. [56]). We build a theoretical framework that incorporates network structures in the MARL framework, and provide computationally efficient algorithms where each agent only needs local information of neighborhood states to learn and to execute the policy.

Secondly, our work builds the theoretical foundation for the practically popular scheme of centralized-training-decentralized-execution (CTDE) (Lowe et al. [40], Rashid et al. [47], Vadori et al. [52], Yang et al. [57]). The CTDE framework is first proposed in Lowe et al. [40] to learn optimal policies in cooperative games with two steps: the first step is to train a global policy for the central controller, and the second step is to decompose the central policy (i.e., a large Q-table) into individual policies so that individual agent can apply the decomposed/decentralized policy after training. Despite the popularity of CTDE, however, there has been no theoretical study as to when the Q-table can be decomposed and when the truncation error can be controlled, except for a heuristic argument by Lowe et al. [40] for large N with local observations. Our paper analyzes for the first time with theoretical guarantee that applying our algorithm to this CTDE paradigm yields a near-optimal sample complexity, when there is a network structure among agent states. Moreover, our algorithm, which is easier to scale-up, improves the centralized training step with a localized training. To differentiate our approach from the CTDE scheme, we call it localized-training-decentralized-execution (LTDE).

Notation. For a set X , denote R X = {f : X → R} as the set of all real-valued functions on X . For each f ∈ R X , define f ∞ = sup x∈X |f (x)| as the sup norm of f . In addition, when X is finite, denote |X | as the size of X , and P(X ) as the set of all probability measures on X : P(X ) = {p : p(x) ≥ 0, x∈X p(x) = 1}, which is equivalent to the probability simplex in R |X | .

[N ] := {1, 2, · · · , N }. For any µ ∈ P(X ) and a subset Y ⊂ X , let µ(Y) denote the restriction of the vector µ on Y, and let P(Y) denote the set {µ(Y) : µ ∈ P(X )}. For x ∈ R d , d ∈ N, denote x 2 as the L 2 -norm of x and x ∞ as the L ∞ -norm of x.


Mean-field MARL with Local Dependency

The focus of this paper is to study a cooperative multi-agent system with a network of agent states, which consists of nodes representing states of the agents and edges by which states are connected. In this system, every agent is only allowed to move from her present state to its connecting states. Moreover, she is assumed to only observe (realistically) partial information of the system on an aggregated level. Mean-field theory provides efficient approximations when agents only observe aggregated information, and has been applied in stochastic systems with large homogeneous agents such as financial markets (Carmona et al. [9], Lacker and Zariphopoulou [34], Hu and Zariphopoulou [27], Casgrain and Jaimungal [12]), energy markets (Germain et al. [21], Aïd et al. [2]), and auction systems (Iyer et al. [29], Guo et al. [26]).


Review of MARL

Let us first recall the cooperative MARL in an infinite time horizon, where there are N agents whose policies are coordinated by a central controller. We assume that both the state space S and the action space A are finite.

At each step t = 0, 1, · · · , the state of agent i (= 1, 2, · · · , N ) is s i t ∈ S and she takes an action a i t ∈ A. Given the current state profile s s s t = (s 1 t , · · · , s N t ) ∈ S N and the current action profile a a a t = (a 1 t , · · · , a N t ) ∈ A N of N agents, agent i will receive a reward r i (s s s t , a a a t ) and her state will change to s i t+1 according to a transition probability function P i (s s s t , a a a t ). A Markovian game further restricts the admissible policy for agent i to be of the form a i t ∼ π i t (s s s t ). That is, π i t : S N → P(A) maps each state profile s s s ∈ S N to a randomized action, with P(A) the space of all probability measures on space A.

In this cooperative MARL framework, the central controller is to maximize the expected discounted accumulated reward averaged over all agents. That is to find
V (s s s) = sup π π π 1 N N i=1 v i (s s s, π π π), (2.1) where v i (s s s, π π π) = E ∞ t=0 γ t r i (s s s t , a a a t ) s s s 0 = s s s (2.2)
is the accumulated reward for agent i, given the initial state profile s s s 0 = s s s and policy π π π = {π π π t } ∞ t=0 with π π π t = (π 1 t , . . . , π N t ). Here γ ∈ (0, 1) is a discount factor, a i t ∼ π i t (s s s t ), and s i t+1 ∼ P i (s s s t , a a a t ). The corresponding Bellman equation for the value function (2.1) is
V (s s s) = sup a a a∈A N E 1 N N i=1 r i (s s s, a a a) + γE s s s ∼P P P (s s s,a a a) [V (s s s )] ,(2.3)
with the population transition kernel P P P = (P 1 , · · · , P N ). The value function can be written as consisting of the expected reward from taking action a a a at state s s s and then following the optimal policy thereafter. The Bellman equation for the Q-function, defined from S N × A N to R, is given by
Q(s s s, a a a) = E 1 N N i=1
r i (s s s, a a a) + γE s s s ∼P P P (s s s,a a a) sup a a a ∈A N Q(s s s , a a a ) .

(2.5)

One can thus retrieve the optimal (stationary) control π * (s s s, a a a) (if it exists) from Q(s s s, a a a), with π * (s s s) ∈ arg max a a a∈A N Q(s s s, a a a).


Mean-field MARL with Local Dependency

In this system, there are N agents who share a finite state space S and take actions from a finite action space A. Moreover, there is a network on the state space S associated with an underlying undirected graph (S, E), where E ⊂ S × S is the set of edges. The distance between two nodes is defined as the number of edges in a shortest path. For a given s ∈ S, N 1 s denotes the nearest neighbor of s, which consists of all nodes connected to s by an edge and includes s itself; and N k s denotes the k-hop neighborhood of s, which consists of all nodes whose distance to s is less than or equal to k, including s itself. For simplicity, we use N s := N 1 s . From agent i's perspective, agents in her neighborhood N s i t change stochastically over time. To facilitate mean-field approximation to this system, assume throughout the paper that the agents are homogeneous and indistinguishable. In particular, at each step t = 0, 1, · · · , if agent i at state s i t ∈ S takes an action a i t ∈ A, then she will receive a stochastic reward which is uniformly upper bounded by r max such that
r i (s s s t , a a a t ) :=r s i t , µ t (N s i t ), a i t ≤ r max , i ∈ [N ]; (2.6)
and her state will change to a neighboring state s i t+1 ∈ N s i t according to a transition probability such that
s i t+1 ∼ P i (s s s t , a a a t ) :=P · s i t , µ t (N s i t ), a i t , i ∈ [N ], (2.7) where µ t (·) = N i=1 1(s i t =·) N ∈ P N (S) := µ ∈ P(S) : µ(s) ∈ 0, 1 N , 2 N , · · · , N −1 N , 1
for all s ∈ S is the empirical state distribution of N agents at time t, with N · µ t (s) the number of agents in state s at time t, and µ t (N s i t ) the restriction of µ t on the 1-hop neighbor of s i t . (2.6)-(2.7) indicate that the reward and the transition probability of agent i at time t depend on both her individual information (a i t , s i t ) and the mean-field of her 1-hop neighborhood µ t (N s i t ), in an aggregated yet localized format: aggregated or mean-field meaning that agent i depends on other agents only through the empirical state distribution; localized meaning that agent i depends on the mean-field information of her 1-hop neighborhood. Intuitive examples of such a setting include traffic-routing, package delivery, data routing, resource allocations, distributed control of autonomous vehicles and social economic systems.

Policies with partial information. To incorporate the element of partial or limited information into this mean-field MARL system, consider the following individual-decentralized policies
a i t ∼ π i (s s s t ) := π s i t , µ t (s i t ) ∈ P(A), i ∈ [N ],(2.8)
and denote u as the admissible policy set of all such policies. Note that for a given mean-field information µ t , π(·, µ t (·)) : S → P(A) maps the agent state to a randomized action. That is, the policy of each agent is executed in a decentralized manner and assumes that each agent only has access to the population information in her own state. This is more realistic than centralized policies which assume full access to the state information of all agents.

Value function and Q-function. The goal for this mean-field MARL is to maximize the expected discounted accumulated reward averaged over all agents, i.e.,
V (µ) := sup π∈u V π (µ) = sup π∈u 1 N N i=1 E ∞ t=0 γ t r s i t , µ t (N s i t ), a i t µ 0 = µ , (MF-MARL)
subject to (2.6)-(2.8) with a discount factor γ ∈ (0, 1). The mean-field assumption leads to the following definition of the corresponding Q-function for (MF-MARL) on the measure space:
Q(µ, h) : = E N i=1 1 N r(s i 0 , µ(N s i 0 ), a i 0 ) s s s 0 , a a a 0
Expected reward of taking a a a0 = (a 1 0 , · · · , a N 0 )
+ E s i 1 ∼P · s i 0 , µ(N s i 0 ), a i 0 ∞ t=1 γ t N i=1 1 N r(s i t , µ t (N s i t ), a i t ) a i t ∼ π t
Expected reward of playing optimally thereafter a i t ∼ π t , (2.9)
where µ(·) = N i=1 1(s i 0 =·) N is the initial empirical state distribution and h(s)(a) = N i=1 1(s i 0 =s,a i 0 =a) N i=1 1(s i 0 =s)
is a "decentralized" policy representing the proportion of agents in state s that takes action a. Specifically, given µ ∈ P N (S), s ∈ S, and the N · µ(s) agents in state s,
h(s) ∈ P N ·µ(s) (A) := ς ∈ P(A) : ς(a) ∈ {0, 1 N · µ(s) , · · · , N · µ(s) − 1 N · µ(s) } for all a ∈ A ⊂ P(A),
where ς in P N ·µ(s) (A) is an empirical action distribution of N · µ(s) agents in state s, and ς(a) is the proportion of agents taking action a ∈ A among all N · µ(s) agents in state s. Furthermore, for a given s ∈ S, denote P N ·µ(s) (A) the set of all admissible "decentralized" policies h(s)(·); and for a given µ ∈ P N (S), denote the product of P N ·µ ( Note that Q(µ, h) defined in (2.9) is invariant with respect to the order of the elements in s s s 0 and a a a 0 . More critically, the input dimension of the Q-function defined in (2.9) is independent from the number of agents in the system, hence is easier to scale up in a large population regime. This differs from the the input dimension of the Q-function in (2.4), which grows exponentially with respect to the number of agents, the main culprit of the curse of dimensionality for MARL algorithms.


Analysis of MF-MARL with Local Dependency

The theoretical study of this mean-field MARL with local dependency (Section 2.2) consists of three key components, which are crucial for subsequent algorithm design and convergence analysis: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents. This reformulation leads to the decomposition of the Q-function and the value function according to states, facilitating updating the consequent team Q-function in a localized fashion (Section 3.1); the second is the Bellman equation for the value function and the Q-function on the probability measure space (Section 3.2); the third is the exponential decay property of the team Q-function, enabling its approximation with a truncated version of a much smaller dimension and yet with a controllable approximation error (Section 3.3).


Markov Decision Process (MDP) on Network of States

This section shows that the mean-field MARL (2.6)-(2.8) can be reformulated in an MDP framework by exploiting the network structure of states. This reformulation leads to the decomposition of the Q-function, facilitating more computationally efficient updates.

The key idea is to utilize the homogeneity of the agents in the problem set-up and to regroup these N agents according to their states. This regrouping translates (MF-MARL) with N agents into a networked MDP with |S| agents teams, indexed by their states. To see how the policy, the reward function, and the dynamics in this networked Markov decision process are induced by the regrouping approach, recall that there are N · µ(s) agents in state s, each agent i in state s will independently choose action a i ∼ π(s, µ(s)) according to the individual-decentralized policy π(s, µ(s)) ∈ P(A) in (2.8). Therefore the empirical action distribution of {a 1 , · · · , a N ·µ(s) } is a random variable taking values from P N ·µ(s) (A), the set of empirical action distributions with N · µ(s) agents. Moreover, for any h(s) ∈ P N ·µ(s) (A), we have
P h(s) is the empirical action distribution of {a 1 , · · · , a N ·µ(s) }, a i i.i.d ∼ π(s, µ(s)) = P for each a ∈ A, a appears N · µ(s)h(s)(a) times in {a 1 , · · · , a N ·µ(s) }, a i i.i.d ∼ π(s, µ(s)) = (N · µ(s))! a∈A (N · µ(s)h(s)(a))! a∈A π(s, µ(s))(a) N ·µ(s)h(s)(a)
.
(3.1)
Here h(s)(a) denotes the proportion of agents taking action a among all agents in state s, with last equality derived from the multinomial distribution with parameters N · µ(s) and π(s, µ(s)). Now, clearly each individual-decentralized policy π(s, µ(s)) ∈ P(A) in (2.8) induces a teamdecentralized policy of the following form:
Π s (h(s) | µ(s)) = (N · µ(s))! a∈A (N · µ(s)h(s)(a))! a∈A π(s, µ(s))(a) N ·µ(s)h(s)(a) ,(3.2)
where h(s) ∈ P N ·µ(s) (A). Conversely, given a team-decentralized policy Π s ( · | µ(s)), one can recover the individual-decentralized policy π(s, µ(s)) by choosing appropriate h(s) ∈ P N ·µ(s) (A) and querying the value of Π s (h(s) | µ(s)): let h i (s) = δ ai be the Dirac measure with a i ∈ A, which is an action distribution such that all agents in state s take action a i . By
(3.2), Π s (h i (s) | µ(s)) = (π(s, µ(s))(a i )) N ·µ(s) , implying π(s, µ(s))(a i ) = (Π(h i (s)|µ(s)) 1
N ·µ(s) . Next, given µ ∈ P N (S) and h ∈ H N (µ) = {h : h(s) ∈ P N ·µ(s) (A), ∀s ∈ S}, the set of empirical action distributions on every state, if we define
Π(h | µ) := s∈S Π s (h(s) | µ(s)), (3.3)
then u, the admissible policy set of individual-decentralized policies in the form of (2.8), is now replaced by U, the set of all team-decentralized policies Π induced from π ∈ u through (3.2) and (3.3). In addition, denote the set of all state-action distribution pairs as
Ξ := ∪ µ∈P N (S) {ζ = (µ, h) : h ∈ H N (µ)},(3.4)
Moreover, from the team perspective, the transition probability in (2.7) can be viewed as a Markov process of µ t and h t ∈ H N (µ t ) with an induced transition probability P N from (2.7) such that
µ t+1 ∼ P N (· | µ t , h t ). (3.5)
It is easy to verify that for a given state s ∈ S, µ t+1 (s) only depends on µ t (N 2 s ), the empirical distribution in the 2-hop neighborhood of s, and h t (N s ).

Finally, given µ(N s ) ∈ P N (N s ), an empirical distribution restricted to the 1-hop neighborhood of s, one can define a localized team reward function for team s from P N ·µ(s) (A) to R as
r s (µ(N s ), h(s)) = a∈A r(s, µ(N s ), a)h(s)(a),(3.6)
which depends on the state s and its 1-hop neighborhood; and define the maximal expected discounted accumulative localized team rewards over all teams as
V (µ) := sup Π∈U V Π (µ) = sup Π∈U E ∞ t=0 s∈S γ t r s (µ t (N s ), h t (s)) µ 0 = µ . (3.7)
With all these key elements, one can establish the equivalence between maximizing the reward averaged over all agents in (MF-MARL) and maximizing the localized team reward summed over all teams in (3.7), and can thus reformulate the (MF-MARL) problem as an equivalent MDP of (3.2)-(3.7) with |S| teams, the latter denoted as (MF-DEC-MARL). (The proof is detailed in Appendix A). That is, Lemma 3.1 (Value function and Q-function decomposition)
V (µ) = V (µ) = sup Π∈U s∈S V Π s (µ), (3.8) where h t ∼ Π(· | µ t ), µ t+1 ∼ P N (· | µ t , h t ), and V Π s (µ) = E ∞ t=0 γ t r s (µ t (N s ), h t (s)) µ 0 = µ (3.9)
is called the value function under policy Π for team s. Similarly,
Q Π (µ, h) : = E ∞ t=0 γ t s∈S r s (µ t (N s ), h t (s)) µ 0 = µ, h 0 = h = s∈S Q Π s (µ, h), (3.10) where Q Π s (µ, h) = E ∞ t=0 γ t r s (µ t (N s ), h t (s)) µ 0 = µ, h 0 = h , (3.11)
is the Q-function under policy Π for team s, called team-decentralized Q-function.

The decomposition for the Q-function in (3.10) is one of the key elements to allow for approximation of Q Π s (µ, h) by a truncated Q-function defined on a smaller space and updated in a localized fashion; it is useful for designing sample-efficient learning algorithms and for parallel computing, as will be clear in the next Section 3.3.


Bellman equation for Q-function.

This section builds the second block for reinforcement learning algorithms, the Bellman equation for Q-function. Indeed, the Bellman equation for Q(µ, h) can be derived following a similar argument in Gu et al. [23], after establishing the dynamic programming principle on an appropriate probability measure space.


Lemma 3.2 (Bellman Equation for Q-function)

The Q-function defined in (2.9) satisfies:
Q(µ, h) = E N i=1 1 N r(s i 0 , µ(N s i 0 ), a i 0 ) s s s 0 , a a a 0 + γE s i 1 ∼P · s i 0 , µ(N s i 0 ), a i 0 sup h ∈H N (µ1) Q (µ 1 , h ) . (3.12) with µ 1 (·) = N i=1 1(s i 1 =·) N the empirical state distribution at time 1.
Note that the Bellman equation (3.12) is for the Q-function defined in (2.9) for general meanfield MARL. In order to enable the localized-training-decentralized-execution for computational efficiency, one needs to consider the decomposition of Q-function (3.10) and the updating rule based on the team-decentralized Q-function (3.11). The corresponding Bellman equation for the team-decentralized Q-function (3.11) is:
Lemma 3.3 Given a policy Π ∈ U, Q Π s defined in (3.11) is the unique solution to the Bellman equation Q Π s = T Π s Q Π s , with T Π s the Bellman operator taking the form of T Π s Q Π s (µ, h) = E µ ∼P N (· | µ,h), h ∼Π(· | µ) r s (µ, h) + γ · Q Π s (µ , h ) , ∀(µ, h) ∈ Ξ. (3.13)
These Bellman equations are the basis for general Q-function-based algorithms in mean-field MARL.


Exponential Decay of Q-function

This section will show that the team-decentralized Q-function Q Π s (µ, h) has an exponential decay property. This is another key element to enable an approximation to Q Π s by a localized Qfunction Q Π s (µ(N k s ), h(N k s )), and to guarantee the scalability and sample efficiency of subsequent algorithm design.

To establish the exponential decay property of the Q-function (3.11), first recall that N k s is the set of k-hop neighborhood of state s, and define N −k s = S/N k s as the set of states that are outside of s'th k-hop neighborhood. Next, rewrite any given empirical state distribution
µ ∈ P N (S) as µ(N k s ), µ(N −k s ) , and similarly, h ∈ H N (µ) as h(N k s ), h(N −k s ) .

Definition 3.4

The Q Π is said to have (c, ρ)-exponential decay property, if for any s ∈ S and
any Π ∈ U, (µ, h), (µ , h ) ∈ Ξ with µ(N k s ) = µ (N k s ) and h(N k s ) = h (N k s ) Q Π s µ(N k s ), µ(N −k s ), h(N k s ), h(N −k s ) − Q Π s µ(N k s ), µ (N −k s ), h(N k s ), h (N −k s ) ≤ cρ k+1 .
Note that the exponential decay property is defined for the team-decentralized Q-function Q Π s , instead of the centralized Q-function Q Π . The following Lemma provides a sufficient condition for the exponential decay property. Its proof is given in Appendix B.

Lemma 3.5 When the reward r s in (3.6) is uniformly upper bounded by r max > 0, for any
s ∈ S, Q Π s satisfies the rmax 1−γ , √ γ -exponential decay property.
The exponential decay property implies that for a given state s ∈ S, the dependence of Q Π s on other states decays quickly with respect to its distance from state s. It motivates and enables the approximation of Q Π s (µ, h) by a truncated function which only depends on µ(N k s ) and h(N k s ), especially when k is large and ρ is small. Specifically, consider the following class of localized Q-functions,
Q Π s µ(N k s ), h(N k s ) = µ(N −k s ),h(N −k s ) w s µ(N −k s ), h(N −k s ); µ(N k s ), h(N k s ) · Q Π s µ(N k s ), µ(N −k s ), h(N k s ), h(N −k s ) , (Local Q-function) where w s µ(N −k s ), h(N −k s ); µ(N k s ), h(N −k s ) are any non-negative weights of µ(N −k s ),h(N −k s ) w s µ(N −k s ), h(N −k s ); µ(N k s ), h(N k s ) = 1
for any µ(N k s ) and h(N k s ). Then, direct computation yields the following proposition. Proposition 3.6 Let Q Π s be any localized Q-function in the form of (Local Q-function). Assume the (c, ρ)-exponential decay property in Definition 3.4 holds, then for any µ ∈ P N (S) and h ∈ H N (µ),
Q Π s µ(N k s ), h(N k s ) − Q Π s (µ, h) ≤ cρ k+1 . (3.14)
Moreover, (3.14) holds independent of the weights in (Local Q-function).

Note that given a team-decentralized Q-function Q Π s , its localized version Q Π s only takes µ(N k s ), h(N k s ) as inputs, and Q Π s µ(N k s ), h(N k s ) is defined as a weighted average of Q Π s over all (µ, h)-pairs which agree with µ(N k s ), h(N k s ) in the k-hop neighborhood of s. Although the localized Q-function Q Π s may vary according to different choices of the weights, by the exponential decay property, every Q Π s approximates Q Π s with uniform error and requires a smaller dimension of input.

Remark 3.7 (Exponential Decay Property) In a discounted reward setting (2.1), the exponential decay property follows directly from the fact that the discount factor γ ∈ (0, 1) and the local dependency structure in (3.2)-(3.7). For problems of finite-time or infinite horizons with ergodic reward functions, this property can be established by imposing additional Lipschitz condition on the transition kernel. (See Qu et al. [44], Theorem 1 for network of heterogeneous agents and γ = 1).

It is also worth pointing out that the exponential decay property has been extensively explored in random graphs (e.g., Gamarnik [19], Gamarnik et al. [20]) and for analysis of network of agents in Qu et al. [44] and Lin et al. [37].


Algorithm Design

The three key analytical components for problem (MF-DEC-MARL) in previous sections pave the way for designing efficient learning algorithms. In this section, we propose and analyze a decentralized neural actor-critic algorithm, called LTDE-Neural-AC.

Our focus is the localized Q-function Q Π s (µ(N k s ), h(N k s )), the approximation to Q Π s with a smaller input dimension. First, this localized Q-function Q Π s and the team-decentralized policy Π s will be parameterized by two-layer neural networks with parameters ω s and θ s respectively (Section 4.2). Next, these neural network parameters θ = {θ s } s∈S and ω = {ω s } s∈S are updated via an actor-critic algorithm in a localized fashion (Section 4.3): the critic aims to find a proper estimate for the localized Q-function under a fixed policy (parameterized by θ), while the actor computes the policy gradient based on the localized Q-function, and updates θ by a gradient step.

These networks are updated locally requiring only information of the neighborhood states during the training phase; afterwards agents in the system will execute these learned decentralized policies which requires only information of the agent's current state. This localized training and decentralized execution enables efficient parallel computing especially for a large shared state space.

Moreover, over-parameterization of neural networks avoids issues of nonconvexity and divergence associated with the neural network approach, and ensures the global convergence of our proposed LTDE-Neural-AC algorithm.


Basic Set-up

Policy parameterization. To start, let us assume that at state s the team-decentralized policy Π θs s is parameterized by θ s ∈ Θ s . Further denote θ := {θ s } s∈S , Θ := s∈S Θ s , Π θ := s∈S Π θs s , and Π := {Π θ : θ ∈ Θ} as the class of admissible policies parameterized by the parameter space {θ : θ ∈ Θ}.

Initialization. Let us also assume that the initial state distribution µ 0 of N agents is sampled from a given distribution P 0 over P N (S), i.e., µ 0 ∼ P 0 ; and define the expected total reward function J(θ) under policy Π θ by
J(θ) = E µ0∼P0 [ V Π θ (µ 0 )]. (4.1)
Visitation measure. Denote ν θ as the stationary distribution on Ξ of the Markov process (3.5) induced by Π θ .

Similar to the single-agent RL problem (Agarwal et al. [1], Fu et al. [18]), each admissible policy Π θ induces a visitation measure σ θ (µ, h) on Ξ describing the frequency that policy Π θ visits (µ, h), with
σ θ (µ, h) := (1 − γ) · ∞ t=0 γ t · P µ t = µ, h t = h | Π θ , (4.2) where µ 0 ∼ P 0 , h t ∼ Π θ (· | µ t ), and µ t+1 ∼ P N (· | µ t , h t ).
Policy gradient theorem. In order to find the optimal parameterized policy Π θ which maximizes the expected total reward function J(θ), the policy optimization step will search for θ ∈ Θ along the gradient direction ∇J(θ). Note that computing the gradient ∇J(θ) depends on both the action selection, which is directly determined by Π θ , and the visitation measure σ θ in (4.2), which is indirectly determined by Π θ . A simple and elegant result called the policy gradient theorem (Lemma 4.1) proposed in Sutton et al. [51], reformulates the gradient ∇J(θ) in terms of Q Π θ in (3.10) and ∇ log Π θ (h | µ) under the visitation measure σ θ . This result simplifies the gradient computation significantly, and is fundamental for actor-critic algorithms.
Lemma 4.1 (Sutton et al. [51]) ∇J(θ) = 1 1−γ E σ θ Q Π θ (µ, h)∇ log Π θ (h | µ) .
Now, direct implementation of the actor-critic algorithm with the centralized policy gradient theorem in Lemma 4.1 suffers from high sample complexity due to the dimension of the Qfunction. Instead, we will show that the exponential decay property of Q-function allows efficient approximation of the policy gradient via localization and hence a scalable algorithm to solve (MF-MARL).


Neural Policy and Neural Q-function

We now turn to the localized Q-function Q Π s (µ(N k s ), h(N k s )) (i.e., the approximation of Q Π s ) and the team-decentralized policy Π s , and their parameterization by two-layer neural networks. We emphasize that the parameterization framework in this section can be extended to any neural-based single-agent algorithms with convergence guarantee.

Two-Layer Neural Network. For any input space X ⊂ R dx with dimension d x ∈ N, a two-layer neural network f (x; W, b) with input x ∈ X and width M ∈ N takes the form of
f (x; W, b) = 1 √ M M m=1 b m · ReLU (x · [W ] m ) . (4.3)
Here the scaling factor 1 √ M called the Xavier initialization (Glorot and Bengio [22]) ensures the same input variance and the same gradient variance for all layers; the activation function ReLU :
R → R, defined as ReLU(u) = 1{u > 0} · u; b={b m } m∈[M ] and W = [W ] 1 , . . . , [W ] M ∈ R M ×dx in (4.
3) are parameters of the neural network. Taking advantage of the homogeneity of ReLU (i.e., ReLU(c · u) = c · ReLU(u) for all c > 0 and u ∈ R), we adopt the usual trick (Cai et al. [6], Wang et al. [53], Allen-Zhu et al. [3]) to fix b throughout the training and only to update W in the sequel. Consequently, denote
f (x; W, b) as f (x; W ) when b m = 1 is fixed. [W ] m is initialized according to a multivariate normal distribution N (0, I dx /d x ), where I dx is the identity matrix of size d x .
Neural Policy. For each s ∈ S, denote the tuple ζ s = (µ(s), h(s)) ∈ R d ζs for notational simplicity, where d ζs := 1 + |A| is the dimension of ζ s . Given the input ζ s = (µ(s), h(s)) and parameter W = θ s in the two-layer neural network f (·; θ s ) in (4.3), the team-decentralized policy Π θs s , called the actor, is parameterized in the form of an energy-based policy ,
Π θs s (h(s) | µ(s)) = exp[τ · f ((µ(s), h(s)); θ s )] h (s)∈P N ·µ(s) (A) exp [τ · f ((µ(s), h (s)); θ s )]
, (4.4) where τ is the temperature parameter and f is the energy function.

To study the policy gradient for (4.4), let us first define a class of feature mappings that is consistent with the representation of two-layer neural networks. This connection between the gradient of a two-layer ReLU neural network and the feature mapping defined in (4.6) is crucial in the convergence analysis of Theorems 5.4 and 5.10. Specifically, rewrite the two-layer neural network in (4.3) as [
f (ζ s ; θ s ) = 1 √ M M m=1 ReLU ζ s [θ s ] m = 1 √ M M m=1 1 ζ s [θ s ] m > 0 · ζ s [θ s ] m . := φ θs (ζ s ) θ s .φ θs ] m (ζ s ) = 1 √ M · 1 ζ s [θ s ] m > 0 · ζ s . (4.6)
That is, the two-layer neural network f (ζ s ; θ s ) may be viewed as the inner product between the feature φ θs (ζ s ), and the neural network parameters θ s . Since f (ζ s ; θ s ) is almost everywhere differentiable with respect to θ s , we see ∇ θs f (ζ s ; θ s ) = φ θs (ζ s ). Furthermore, define a "centered" version of the feature φ θs such that
Φ(θ, s, µ, h) := φ θs (µ(s), h(s)) − E h(s) ∼Π θs s (·|µ(s)) [φ θs (µ(s), h (s))] . (4.7)
Note that when policy Π θ takes the energy-based form (4.4), Φ = 1 τ ∇ θ log Π θ . Therefore, Lemma 4.2 For any θ ∈ Θ, s ∈ S, µ ∈ P N (S) and h ∈ H N (µ), Φ(θ, s, µ, h) 2 ≤ 2, and
∇ θs J (θ) = τ 1 − γ · E σ θ Q Π θ (µ, h) · Φ(θ, s, µ, h) . (4.8)
Moreover, for each s ∈ S, define the following localized policy gradient
g s (θ) = τ 1 − γ E σ θ   y∈N k s Q Π θ y (µ(N k y ), h(N k y ) · Φ(θ, s, µ, h)   ,(4.9)
with Q Π θ s in (Local Q-function) satisfying the (c, ρ)-exponential decay property, then there exists a universal constant c 0 > 0 such that
g s (θ) − ∇ θs J(θ) ≤ c 0 τ |S| 1 − γ ρ k+1 . (4.10)
Neural Q-function. Note Q Π θ s in (Local Q-function) is unknown a priori. To obtain the localized policy gradient (4.9), the neural network (4.3) to parameterize Q Π θ s is taken as:
Q s (µ(N k s ), h(N k s ); ω s ) = f ((µ(N k s ), h(N k s )); ω s ).
This Q s is called the critic. For simplicity, denote ζ k s = (µ(N k s ), h(N k s )), with d ζ k s the dimension of ζ k s .


Actor-Critic

Critic Update. For a fixed policy Π θ , it is to estimate Q Π θ s of (Local Q-function) by a twolayer neural network Q s ( · ; ω s ), where Q Π θ s serves as an approximation to the team-decentralized Q-function Q Π θ s . To design the update rule for Q Π θ s , note that the Bellman equation
(3.13) is for Q Π θ s in- stead of Q Π θ s . Indeed, Q Π θ s takes (µ, h) as the input while Q Π θ s
takes the partial information (µ(N k s ), h(N k s )) as the input. In order to update parameter ω s , we substitute (µ(N k s ), h(N k s )) for the state-action pair in the Bellman equation (3.13). It is therefore necessary to study the error of using (µ(N k s ), h(N k s )) as the input. Specifically, given a tuple (µ t , h t , r s (µ t (N s ), h t (s)), µ t+1 , h t+1 ) sampled from the stationary distribution ν θ of adopting policy Π θ , the parameter ω s will be updated to minimize the error:
(δ s,t ) 2 = Q s (µ t (N k s ), h t (N k s ); ω s ) − r s (µ t (N s ), h t (s)) − γ · Q s (µ t+1 (N k s ), h t+1 (N k s ); ω s ) 2 .
Estimating δ s,t depends only on µ t (N k s ), h t (N k s ) and can be collected locally. (See Theorem 5.4).

The neural critic update takes the iterative forms of
ω s (t + 1/2) ← ω s (t) − η critic · δ s,t · ∇ ωs Q s (µ t (N k s ), h t (N k s ); ω s ), (4.11) ω s (t + 1) ← arg min ω∈B critic s ω − ω s (t + 1/2) 2 , (4.12) ω s ← (t + 1)/(t + 2) ·ω s + 1/(t + 2) · ω s (t + 1),(4.13)
in which η critic is the learning rate. Here (4.11) is the stochastic semigradient step, (4.12) is a projection to the parameter space B critic
s := ω s ∈ R M ×d ζ k s : ω s − ω s (0) ∞ ≤ R/ √
M for some R > 0, and (4.13) is the averaging step. This critic update is summarized in Algorithm 1. Sample (µ t , h t , {r s (µ t (N s ), h t (s))} s∈S , µ t , h t ) from the stationary distribution ν θ of Π θ .


5:

for s ∈ S do 6:
Denote ζ k s,t = (µ t (N k s ), h t (N k s )), ζ k s,t = (µ t (N k s ), h t (N k s )).

7:

Residual calculation:
δ s,t ← Q s (ζ k s,t ; ω s (t)) − r s (µ t (N s ), h t (s)) − γ · Q s (ζ k s,t
; ω s (t)).


8:

Temporal difference update: 9:
ω s (t + 1/2) ← ω s (t) − η critic · δ s,t · ∇ ωs Q s (ζ k s,t
; ω s (t)).


10:

Projection onto the parameter space: ω s (t + 1) ← arg min ω∈B critic s ω − ω s (t + 1/2) 2 .


11:

Averaging the output:ω s ← t+1 t+2 ·ω s + 1 t+2 · ω s (t + 1).


12:

end for 13: end for 14: Output: Q s ( · ;ω s ), ∀s ∈ S.

Actor Update. At the iteration step t, a neural network estimation Q s ( · ;ω s ) is given for the localized Q-function Q Π θ(t) s under the current policy Π θ(t) . Let {(µ l , h l )} l∈ [B] be samples from the state-action visitation measure σ θ(t) of (4.2), and define an estimator Φ(θ, s, µ l , h l ) of Φ(θ, s, µ l , h l ) in (4.7):
Φ(θ, s, µ l , h l ) = φ θs (µ l (s), h l (s)) − E Π θs s [φ θs (µ l (s), h (s))]
. By Lemma 4.2, one can compute the following estimator of g s (θ(t)) defined in (4.9),
g s (θ(t)) = τ (1 − γ)B l∈[B]   y∈N k s Q y µ l (N k y ), h l (N k y );ω y · Φ(θ(t), s, µ l , h l )   .
(4.14)

This estimators g s in (4.14) only depends locally on {(µ l , h l )} l∈ [B] . Hence g and Φ can be computed in a localized fashion after the samples are collected. Similar to the critic update, θ s (t) is updated by performing a gradient step with g s , and then projected onto the parameter space B actor
s := θ s ∈ R M ×d ζs : θ s − θ s (0) ∞ ≤ R/ √ M .
This actor update is summarized in Algorithm 2.

Sampling from ν θ and the Visitation Measure σ θ . In Algorithms 1 and 2, it is assumed that one can sample independently from the stationary distribution ν θ and the visitation measure σ θ , respectively. Such an assumption of sampling from ν θ can be relaxed by either sampling from a rapidly-mixing Markov chain mixing, with weakly-dependent sequence of samples (Bhandari et al. [4]), or by randomly picking samples from replay buffers consisting of long trajectories, with reduced correlation between samples.

To sample from the visitation measure σ θ and computing the unbiased policy gradient estimator, Konda and Tsitsiklis [33] suggests introducing a new MDP such that the next state is sampled from the transition probability with probability γ, and from the initial distribution with probability 1 − γ. Then the stationary distribution of this new MDP is exactly the visitation measure. Alternatively, Liu et al. [39] proposes an importance-sampling-based algorithm which enables off-policy evaluation with low variance. .


Algorithm 2 Localized-Training-Decentralized-Execution Neural Actor-Critic


5:

Output Q s ( · ;ω s ) using Algorithm 1 with the inputs: policy Π θ , width of the neural network M , radius of the constraint set R, number of iterations T critic , learning rate η critic and localization parameter k.


6:

Sample {µ l , h l } l∈[B] from the state-action visitation measure σ θ (4.2) of Π θ .


7:

for s ∈ S do 8:

Compute the local gradient estimator g s (θ(t)) using (4.14).


9:

Policy update: θ s (t + 1/2) ← θ s (t) + η actor · g s (θ(t)) 10:

Projection onto the parameter space: θ s (t + 1) ← arg min θ∈B actor s θ − θ s (t + 1/2) 2 . 


Convergence of the Critic and Actor Updates

We now establish the global convergence for LTDE-Neural-AC proposed in Section 4.

Convergence of the Critic Update. The convergence of the decentralized neural critic update in Algorithm 1 relies on the following assumptions.


Assumption 5.1 (Action-Value Function Class)For each
s ∈ S, k ∈ N, define F s,k R,∞ = f (ζ k s ) = Q s (ζ k s ; ω s (0)) + 1 v ζ k s > 0 · (ζ k s ) ι(v) dµ(v) : ι(v) ∞ ≤ R , with µ : R d ζ k
s → R the density function of Gaussian distribution N (0, I d ζ k s /d ζ k s ) and Q s (ζ k s ; ω s (0)) the two-layer neural network under the initial parameter ω s (0). We assume that Q Π θ s ∈ F s,k R,∞ .

Assumption 5.2 (Regularity of ν θ and σ θ ) There exists a universal constant c 0 > 0 such that for any policy Π θ , any α ≥ 0, and any v ∈ R d ζ with v 2 = 1, the stationary distribution ν θ and the state visitation measure σ θ satisfy  
P ζ∼ν θ v ζ ≤ α ≤ c 0 · α, P ζ∼σ θ v ζ ≤ α ≤ c 0 · α.E init Q s ( · ;ω s ) − Q Π θ s (·) 2 L 2 (ν θ ) ≤ O   R 3 d 3/2 ζ k s M 1/2 + R 5/2 d 5/4 ζ k s M 1/4 + r 2 max γ k+1 (1 − γ) 2   , (5.1) where f L 2 (ν θ ) := E ζ∼ν θ [f (ζ) 2 ]
1/2 , and the expectation (5.1) is taken with respect to the random initialization.

Theorem 5.4 indicates the trade-off between the approximation-optimization error and the localization error. The first two terms in (5.1) correspond to the neural network approximationoptimization error, similar to the single-agent case (Cai et al. [6], Cayci et al. [13]). This approximation-optimization error decreases when the width of the hidden layer M increases. Meanwhile, the last term in (5.1) represents the additional error from using the localized information in (4.11), unique for the mean-field MARL case. This localization error and γ k decrease as the number of truncated neighborhood k increases, with more information from a larger neighborhood used in the update. However, the input dimension d ζ k s and the approximationoptimization error will increase if the dimension of the problem increases.

In particular, for a relatively sparse network on S, one can choose k |S| hence d ζ k s d ζ , and Theorem 5.4 indicates the superior performance of the localized training scheme in efficiency over directly approximating the centralized Q-function.

Proof of Theorem 5.4 is presented in Section D.1.

Convergence of the Actor Update. This section establishes the global convergence of the actor update. The convergence analysis consists of two steps. The first step proves the convergence to a stationary point θ; the second step controls the gap between the stationary point θ and the optimality θ * in the overparametrization regime. The convergence is built under the following assumptions and definition.

Assumption 5.5 (Variance Upper Bound) For every t ∈ [T actor ] and s ∈ S, denote ξ s (t) = g s (θ(t)) − E [ g s (θ(t))] with g s (θ(t)) defined in (4.14). Assume there exists Σ > 0 such that E ξ s (t) 2 2 ≤ τ 2 Σ 2 /B. Here the expectations are taken over σ θ(t) given {ω s } s∈S .

Assumption 5.6 (Regularity Condition on σ θ and ν θ ) There exists an absolute constant D > 0 such that for every Π θ , the stationary distribution ν θ and the state-action visitation measure σ θ satisfy
E ν θ (dσ θ /dν θ (µ, h)) 2 ≤ D 2 ,
where dσ θ /dν θ is the Radon-Nikodym derivative of σ θ with respect to ν θ .

Assumption 5.7 (Lipschitz Continuous Policy Gradient) There exists an absolute constant L > 0, such that ∇ θ J(θ) is L-Lipschitz continuous with respect to θ, i.e., for all θ 1 , θ 2 ,
∇ θ J(θ 1 ) − ∇ θ J(θ 2 ) 2 ≤ L · θ 1 − θ 2 2 .
Definition 5.8 θ ∈ B actor is called a stationary point of J(θ) if for all θ ∈ B actor ,
∇ θ J( θ) (θ − θ) ≤ 0. (5.2)

Assumption 5.9 (Policy Function Class) Define a function class
F R,∞ = f (ζ) = s∈S φ θs(0) (ζ s ) θ s (0) + 1 v ζ s > 0 · (ζ s ) ι(v) dµ(v) : ι(v) ∞ ≤ R
where µ : R d ζs → R is the density function of the Gaussian distribution N 0, I d ζs /d ζs and θ(0) is the initial parameter. For any stationary point θ, define the function
u θ (µ, h) := dσ θ * dσ θ (ζ) − dσ θ * dσ θ (µ) + s∈S φ θs (ζ s ) θ s ,
withσ θ the state visitation measure under policy Π θ , and dσ θ * dσ θ , dσ θ * dσ θ the Radon-Nikodym derivatives between corresponding measures. We assume that u θ ∈ F R,∞ for any stationary point θ.

A few remarks are in place for these Assumption 5.5 -Assumption 5.9.

Remark. All these assumptions are counterparts of standard assumption in the analysis of single-agent policy gradient method (Pirotta et al. [43], Xu et al. [54], Xu et al. [55], Zhang et al. [59], Wang et al. [53]).

In particular, Assumption 5.5 and Assumption 5.6 hold if the Markov chain (3.5) mixes sufficiently fast, and the critic Q s ( · ; ω s ) has an upper-bounded second moment under σ θ(t) (Wang et al. [53]). Note that different from Assumption 5.2, where regularity conditions are imposed separately on ν θ and σ θ , Assumption 5.6 imposes the regularity condition directly on the Radon-Nikodym derivative of σ θ with respect to ν θ . This allows the change of measures in the analysis of Theorem 5.10. In general, Assumption 5.2 does not necessarily imply Assumption 5.6. Assumption 5.7 holds when the transition probability and the reward function are both Lipschitz continuous with respect to their inputs (Pirotta et al. [43]), or when the reward is uniformly bounded and the score function ∇ θ Π θ is uniformly bounded and Lipschitz continuous with respect to θ (Zhang et al. [59]).

As for Assumption 5.9, we first emphasize that u θ (µ, h) is a key element in the proof of Theorem 5.10. More specifically, this assumption is motivated by the well-known Performance Difference Lemma (Kakade and Langford [32]) in order to characterize the optimality gap of a stationary point θ. In particular, it guarantees that u θ can be decomposed into a sum of local functions depending on ζ s , and that each local function lies in a rich RKHS (see the discussion after Assumption 5.1).

With all these assumptions, we now establish the rate of convergence for Algorithm 2. Detailed proof is provided in Section D.2.


A Proof of Lemma 3.1

The goal is to show that V (µ) = V (µ), with the former the value function of (MF-MARL) subject to the transition probability P defined in (2.7) under a given individual policy π ∈ u, and the latter the value function of (3.7) subject to the joint transition probability P N defined in (3.5) under the policy Π ∈ U. The proof consists of two steps.

Step 1 shows that V (µ) can be reformulated as a measured-valued Markov decision problem.

Step 2 shows that the measured-valued Markov decision problem from Step 1 is equivalent to V (µ) in (3.7).

Step 1:
Recall that µ t+1 := 1 N N i=1 δ s i t+1 with s i t+1 subject to (2.7)
. First, one can show that µ t is a measure-valued Markov decision process under π. To see this, denote F s t = σ(s 1 t , · · · , s N t ) as the σ-algebra generated by s 1 t , · · · , s N t . Then it suffices to show 
P(µ t+1 | σ(µ t ) ∨ F s t ) = P(µ t+1 | σ(µ t )), P − a.µ t+1 ∼ P N (· | µ t , π). (A.2)
Meanwhile, rewrite V π (µ) in (MF-MARL) by regrouping the agents according to their states Step 2: It suffices to show that (A.2) under π is the same as (3.5) under Π and that V π in (A.3) equals to V Π in (3.7). To see this, denote g, µ = s∈S g(s)µ(s) for any measurable bounded function g : S → R, then
V π (µ) := E ∞ t=0 γ t N i=1 1 N r(s i t , µ t (N s i t ), a i t ) µ 0 = µ , (A.3) = E ∞E g, µ t+1 | σ(µ t ) = 1 N E N i=1 E g(s j t+1 ) | σ(µ t ) ∨ F s t = 1 N s ∈S N i=1 a∈A g(s )P (s | s i t , µ t (N (s i t )), a)π(s i t , µ t (s i t ))(a) = 1 N s ∈S g(s ) s∈S N i=1 1(s i t = s) a∈A P (s | s i t , µ t (N (s i t )), a)π(s i t , µ t (s i t ))(a) = s ∈S g(s ) s∈S µ t (s) a∈A P (s | s, µ t (N (s)), a)π(s, µ t (s))(a) = s ∈S g(s ) s∈S µ t (s) h∈P N ·µ t (s) (A) Π(h | µ t (s)) a∈A P (s | s, µ t (N (s)), a)h(s)(a), (A.4)
where in the last step, the expectation of random variable h(s)(a) with respect to distribution Π(h | µ) is π(s, µ t (s)). And from the last equality, clearly µ t+1 evolves according to transition dynamics P N (·|µ t , h t ) under Π(h t | µ t ). This implies the equivalence of (A.2) and (3.5). As a byproduct, when taking g(s ) = 1(s = s o ) for any fixed s o ∈ S, (A.4) becomes
E µ t+1 (s o )|σ(µ t ) = s∈N (s o ) µ t (s) h∈P N ·µ t (s) (A) Π(h | µ t (s)) a∈A P (s o | s, µ t (N (s)), a)h(s)(a),
where the local structure (2.7) is used. This suggests that µ t+1 (s o ) only depends on µ t (N 2
s o ) and h t (N s o ) since N (s) = N 2 (s o ) for s ∈ N (s o ). Now we show that V π (µ) in (A.3) and V Π (µ) in (3.7) are equal. Take V Π defined in (3.7), V Π (µ) : = E ht∼Π(· | µt), µt+1∼P N (· | µt,ht) ∞ t=0 s∈S γ t r s (µ t (N s ), h t ) µ 0 = µ = E µt+1∼P N (· | µt,ht) ∞ t=0 γ t s∈S E ht∼Π(· | µt) r s (µ t (N s ), h t )|µ t µ 0 = µ = E µt+1∼P N (· | µt,ht) ∞ t=0 γ t s∈S ht∈P N ·µ t (s) (A) r s (µ t (N s ), h t (s))Π(h; π) µ 0 = µ = E µt+1∼P N (· | µt,ht) ∞ t=0 γ t s∈S µ t (s) ht∈P N ·µ t (s) (A) Π(h t | µ t ) a∈A r(s, µ t (N s ), a)h(a) µ 0 = µ = E µt+1∼ P N (· | µt,π) ∞ t=0 γ t s∈S µ t (s) a∈A r(s, µ t (N s ), a)π t (s, µ t (s))(a) µ 0 = µ = V π (µ),
where in the last second step, P N under π is equivalent to P N under Π, and the expectation of
h t (s)(a) with distribution Π(h t | µ t ) is π(s, µ t (s))(a) such that h∈P N ·µ t (s) (A) Π(h t | µ t ) a∈A r(s, µ t (N s ), a)h(a) = E h∼Π(·|µt) a∈A r(s, µ t (N s , a)h(a)
= a∈A r(s, µ t (N s ), a)π t (s, µ t (s))(a).

Finally, the decomposition of V (µ) and Q Π θ (µ, h) according to the states is straightforward. Q.E.D.


B Proof of Lemma 3.5

Let P t,s and P t,s be, respectively, distribution of (µ t (N s ), h t (s)) and (µ t (N s ), h t (s)) under policy Π θ . By localized transition kernel (2.7), it is easy to see that for any given s ∈ S, µ t+1 (s) only depends on µ t (N 2 s ) and h t (N s ). Then by the local dependency, (3.5) can be rewritten as
µ t+1 (s) ∼ P N s (· | µ t (N 2 s ), h t (N s )). (B.1)
Due to the local structure of dynamics (B.1) and local dependence of Π θ , the distribution P t,s , t ≤ k 2 only depends on the initial value (µ(N k s ), h(N k s )). Therefore, P t,s = P t,s , t ≤ k 2 ,
Q Π θ s µ(N k s ), µ(N −k s ), h(N k s ), h(N −k s ) − Q Π θ s µ(N k s ), µ (N −k s ), h(N k s ), h (N −k s ) = ∞ t= k 2 +1 E (µt(Ns),ht(s))∼Pt,s r s (µ t (N s ), h t (s)) − E (µ t (Ns),h t (s))∼P t,s r s (µ t (N s ), h t (s)) ≤ ∞ t= k 2 +1 γ t r max TV(P t,s , P t,s ) ≤ r max 1 − γ γ k 2 +1 ,
where TV(P t,s , P t,s ) is total variation between P t,s and P t,s that is upper bounded by 1.

Q.E.D.


C Proof of Lemma 4.2

For any θ ∈ Θ, s ∈ S, µ ∈ P N (S) and h ∈ H N (µ), it is easy to verify that Φ(θ, s, µ, h) 2 ≤ ζ s 2 ≤ 2, by the definitions of the feature mapping φ in (4.6) and the center feature mapping Φ in (4.7). To prove (4.8), note that by Lemma 4.1 & the definition of energy-based policy Π θs s (4.4),
∇ θs log Π θs s (h(s) | µ(s)) = τ · ∇ θs f ((µ(s), h(s)); θ s ) − τ · E h(s) ∼Π θs (·|µ(s)) [∇ θs f (µ(s), h (s))] = τ · φ θs (µ(s), h(s)) − τ · E h(s) ∼Π θs (·|µ(s)) [φ θs (µ(s), h(s))] = τ · Φ(θ, s, µ, h).
The second equality follows from the fact that ∇ θs f ((µ(s), h(s)); θ s ) = φ θs (µ(s), h(s)). Therefore,
∇ θs J(θ) = τ 1 − γ E σ θ Q Π θ (µ, h) · Φ(θ, s, µ, h) = τ 1 − γ E σ θ   y∈S Q Π θ y (µ, h) · Φ(θ, s, µ, h)   ,
where the second equality is by the decomposition of Q-function in Lemma 3.1.

The proof of (4.9) is based on the exponential decay property in Definition 3.4. Notice that
g s (θ) = 1 1 − γ E σ θ   y∈N k s Q Π θ y (µ(N k y ), h(N k y ) ∇ θs log Π θs (h(s) | µ(s))   = 1 1 − γ E σ θ   y∈S Q Π θ y (µ(N k y ), h(N k y ) ∇ θs log Π θs (h(s) | µ(s))   . (C.1) This is because for all y ∈ N k s , Q Π θ y (µ(N k y ), h(N k y ) is independent of s. Consequently, E σ θ     y ∈N k s Q Π θ y (µ(N k y ), h(N k y )   ∇ θs log Π θs (h(s) | µ(s))   = 0.
Given Lemma 4.1 and (C.1), we have the following bound:
g s (θ) − ∇ θs J(θ) 2 ≤ 1 1 − γ y∈S sup µ∈P N (S), h∈H N (µ) Q Π θ y µ(N k y ), h(N k y ) − Q Π θ y (µ, h) · ∇ θs log Π θs (h(s) | µ(s)) 2 ≤ c 0 τ |S| 1 − γ ρ k+1 .
The last inequality follows from (3.14) and log Π θs (h(s) | µ(s)) 2 = Φ(θ, s, µ, h) 2 ≤ 2 for any µ ∈ P N (S), h ∈ H N (µ). 


D.1.1 Notations

Recall that the set of all state-action (distribution) pairs is denoted as Ξ := ∪ µ∈P N (S) {ζ = (µ, h) : h ∈ H N (µ)}. For any ζ = (µ, h) ∈ Ξ, denote the localized state-action (distribution) pair as ζ k s = (µ(N k s ), h(N k s )). Meanwhile, denote Ξ k s = {ζ k s : ζ ∈ Ξ} as the set of all possible localized state-action (distribution) pairs. Without loss of generality, assume ζ k s 2 ≤ 1 for any ζ k s ∈ Ξ k s . Let d ζ denote the dimension of the space Ξ. Since P N (S) has dimension (|S| − 1) and H N (µ) has dimension |S|(|A| − 1) for any µ ∈ P N (S), the product space Ξ has dimension d ζ = |S||A| − 1. Similarly, one can see that the dimension of the space Ξ k s , denoted by d ζ k s , is at most f (k)|A|, where f (k) := max s∈X |N k s | is the size of the largest k-neighborhood in the graph (S, E).

Let R Ξ and R Ξ k s be the sets of real-valued square-integrable functions (with respect to ν θ ) on Ξ and Ξ k s , respectively. Define the norm · L 2 (ν θ ) on R Ξ by
f L 2 (ν θ ) := E ζ∼ν θ [f (ζ) 2 ] 1/2 , ∀f ∈ R Ξ . (D.1) Note that for any function f ∈ R Ξ k s , a functionf ∈ R Ξ is called a natural extension of f if f (ζ) = f (ζ k s ) for all ζ ∈ Ξ.
Since the natural extension is an injective mapping from R Ξ k s to R Ξ , one can view R Ξ k s as a subset of R Ξ . In addition for a function f ∈ R Ξ k s , we use the same notation f ∈ R Ξ to denote the natural extension of f .

For any closed and convex function class F ⊂ R Ξ , define the project operator Proj F from
R Ξ onto F by Proj F (g) := arg min f ∈F f − g L 2 (ν θ ) . (D.2)
This projection operator Proj F is non-expansive in the sense that
Proj F (f ) − Proj F (g) L 2 (ν θ ) ≤ f − g L 2 (ν θ ) . (D.3)
Recall that for each state s ∈ S, the critic parameter ω s is updated in a localized fashion using information from the k-hop neighborhood of s. Without loss of generality, let us omit the subscript s of ω s in the following presentation, and the result holds for all s ∈ S simultaneously.

Given an initialization ω(0) ∈ R M ×d ζ k s , define the following function class
F R,M = Q 0 (ζ k s ; ω) := 1 √ M M m=1 1 [ω(0)] m ζ k s > 0 ω m ζ k s : ω ∈ R M ×d ζ k s , ω − ω(0) ∞ ≤ R/ √ M . (D.4)
Q 0 ( · ; ω) locally linearizes the neural network Q( · ; ω) (with respect to ω) at ω(0). Any function Q 0 ( · ; ω) ∈ F R,M can be viewed as an inner product between the feature mapping φ ω(0) (·) defined in (4.6) and the parameter ω, i.e. Q 0 ( · ; ω) = φ ω(0) (·) ω. In addition it holds that ∇ ω Q 0 ( · ; ω) = φ ω(0) (·). All functions in F R,M share the same feature mapping φ ω(0) (·) which only depends on the initialization ω(0). Recall the Bellman operator T θ s : R Ξ → R Ξ defined in (3.13),
T θ s Q Π θ s (µ, h) = E µ ∼P N (· | µ,h), h ∼Π θ (· | µ) r s (µ, h) + γ · Q Π θ s (µ , h ) , ∀(µ, h) ∈ Ξ.
The team-decentralized Q-function Q Π θ s in (3.10) is the unique fixed point of T θ s : Q Π θ s = T θ s Q Π θ s . Now given a general parameterized function class F, we aim to learn a Q s ( · ; ω) ∈ F to approximate Q Π θ s by minimizing the following projected mean-squared Bellman error (PMSBE):
min ω PMSBE(ω) = E ζ∼ν θ Q s ( ζ k s ; ω) − Proj F T θ s Q s ( ζ k s ; ω) 2 . (D.5)
In the first step of the convergence analysis, we take F = F R,M (the locally linearized two-layer neural network defined in (D.4)) and consider the following PMSBE:
min ω E ζ∼ν θ Q 0 ( ζ k s ; ω) − Proj F R,M T θ s Q 0 ( ζ k s ; ω) 2 .
(D. 6) We will show in Section D.1.2 that the output of Algorithm 1 converges to the global minimizer of (D.6). Proof. Proof of Lemma D. 1 We first show that the operator T θ s :
R Ξ → R Ξ (3.13) is a γ- contraction in the L 2 (ν θ )-norm. T θ s Q 1 − T θ s Q 2 2 L 2 (ν θ ) = E ζ∼ν θ T θ s Q 1 (ζ) − T θ s Q 2 (ζ) 2 = γ 2 E ζ∼ν θ E Q 1 (ζ ) − Q 2 (ζ ) ζ = (µ , h ), µ ∼ P N (· | ζ), h ∼ Π θ (· | µ ) 2 ≤ γ 2 E ζ∼ν θ E (Q 1 (ζ ) − Q 2 (ζ )) 2 ζ = (µ , h ), µ ∼ P N (· | ζ), h ∼ Π θ (· | µ ) = γ 2 E ζ ∼ν θ [(Q 1 (ζ ) − Q 2 (ζ )) 2 ] = γ 2 Q 1 − Q 2 2 L 2 (ν θ ) ,
where the first inequality follows from Hölder's inequality for the conditional expectation and the third equality stems from the fact that ζ and ζ have the same stationary distribution ν θ . Meanwhile, the projection operator Proj F R,M : R Ξ → F R,M is non-expansive. Therefore, the operator Proj F R,M T θ s : F R,M → F R,M is γ-contraction in the L 2 (ν θ )-norm. Hence Proj F R,M admits a unique fixed point Q 0 ( · ; ω * ) ∈ F R,M . By definition, Q 0 ( · ; ω * ) is the global minimizer of MSPBE that corresponds to the projection onto F R,M in (D.6). 2

Q.E.D. We will show that the function class F R,M will approximately become F s,k R,∞ (defined in Assumption 5.1) as M → ∞, where F s,k R,∞ is a rich reproducing kernel Hilbert space (RKHS). Consequently, Q 0 ( · ; ω * ) will become the global minimum of the MSPBE (D.6) on F s,k R,∞ given Lemma D.1. Moreover, by using similar argument and technique developed in [6,Theorem 4.6], we can establish the convergence of Algorithm 1 to Q 0 ( · ; ω * ) as the following.

Theorem D.2 (Convergence to Q 0 ( · ; ω * )) Set η critic = min{(1 − γ)/8, 1/ √ T critic } in Algorithm 1. Then the output Q s ( · ;ω) of Algorithm 1 satisfies
E init Q s ( · ;ω) − Q 0 ( · ; ω * ) 2 L 2 (ν θ ) ≤ O   R 3 d 3/2 ζ k s √ M + R 5/2 d 5/4 ζ k s 4 √ M + R 2 d ζ k s √ T critic   ,
where the expectation is taken with respect to the random initialization.

The proof of Theorem D.2 is straightforward from [6,Theorem 4.6] and hence omitted.
D.1.3 Convergence to Q Π θ s
Next, we analyze the error between the global minimizer of (D.6) and the team-decentralized Q-function Q Π θ s (defined in (3.10)) to complete the convergence analysis. Different from the single-agent case as in Cai et al. [6], we have to bound an additional error from using the localized information in the critic update, in addition to the neural network approximationoptimization error.

Proof. Proof of Theorem 5.4 First recall that by Lemma 3.5, Q Π θ s satisfies the (c, ρ)-exponential decay property in Definition 3.4, with c = rmax 1−γ , ρ = √ γ. Now, let Q Π θ s be any localized Q-function in (Local Q-function), then
Q Π θ s (ζ) − Q Π θ s (ζ k s ) ≤ cρ k+1 , ∀ζ ∈ Ξ. (D.7)
By the triangle inequality and (a + b) 2 ≤ 2(a 2 + b 2 ),
Q s ( · ;ω) − Q Π θ s (·) 2 L 2 (ν θ ) ≤ Q s ( · ;ω) − Q 0 ( · ; ω * ) L 2 (ν θ ) + Q Π θ s (·) − Q 0 ( · ; ω * ) L 2 (ν θ ) 2 ≤ 2 Q s ( · ;ω) − Q 0 ( · ; ω * ) 2 L 2 (ν θ ) + Q Π θ s (·) − Q 0 ( · ; ω * ) 2 L 2 (ν θ )
.

(D.8)

The first term in (D.8) is studied in Theorem D.2 and it suffices to bound the second term. By interpolating two intermediate terms Q Π θ s and Proj F R,M Q Π θ s , we have
Q Π θ s (·) − Q 0 ( · ; ω * ) L 2 (ν θ ) ≤ Q Π θ s (·) − Q Π θ s (·) L 2 (ν θ ) (I) + Q Π θ s (·) − Proj F R,M Q Π θ s (·) L 2 (ν θ ) (II) + Q 0 ( · ; ω * ) − Proj F R,M Q Π θ s (·) L 2 (ν θ )(III)
.


(D.9)

First, we have (I) ≤ cρ k+1 according to (D.7). To bound (III), we have
(III) = Proj F R,M T θ s Q 0 ( · ; ω * ) − Proj F R,M Q Π θ s (·) L 2 (ν θ ) ≤ Proj F R,M T θ s Q 0 ( · ; ω * ) − Proj F R,M T θ s Q Π θ s (·) L 2 (ν θ ) + Proj F R,M T θ s Q Π θ s (·) − Proj F R,M Q Π θ s (·) L 2 (ν θ ) ≤ γ Q 0 ( · ; ω * ) − Q Π θ s (·) L 2 (ν θ ) + T θ s Q Π θ s (·) − Q Π θ s (·) L 2 (ν θ ) = γ Q 0 ( · ; ω * ) − Q Π θ s (·) L 2 (ν θ ) + Q Π θ s (·) − Q Π θ s (·) L 2 (ν θ ) (I) ≤ γ Q 0 ( · ; ω * ) − Q Π θ s (·) L 2 (ν θ ) + cρ k+1 . (D.
10)

The first line in (D.10) is due to the fact that Q 0 (·; ω * ) is the unique fixed point of the operator Proj F R,M T θ s , (as proved in Lemma D.1); the third line in (D.10) is because the operator Proj F R,M T θ s is a γ-contraction in the L 2 (ν θ ) norm, and Proj F R,M is non-expansive; the fourth line in (D.10) uses the fact that Q Π θ s is the unique fixed point of T θ s ; and the last line comes from the fact that (I) ≤ cρ k+1 . Therefore, combining the self-bounding inequality (D.10) with (D.9) and the bound on (I) gives us
Q Π θ s (·) − Q 0 ( · ; ω * ) L 2 (ν θ ) ≤ 1 1 − γ      2cρ k+1 + Q Π θ s (·) − Proj F R,M Q Π θ s (·) L 2 (ν θ ) (II)     
, and consequently,
Q Π θ s (·) − Q 0 ( · ; ω * ) 2 L 2 (ν θ ) ≤ 1 (1 − γ) 2      8c 2 ρ 2k+2 + 2 Q Π θ s (·) − Proj F R,M Q Π θ s (·) 2 L 2 (ν θ ) (II)      . (D.11)
Plugging (D.11) into (D.8) yields
E init Q s ( · ;ω) − Q Π θ s (·) 2 L 2 (ν θ ) ≤ 2 E init Q s ( · ;ω) − Q 0 ( · ; ω * ) 2 L 2 (ν θ ) + E init Q Π θ s (·) − Q 0 ( · ; ω * ) 2 L 2 (ν θ ) ≤ O   R 3 d 3/2 ζ k s √ M + R 5/2 d 5/4 ζ k s 4 √ M + R 2 d ζ k s √ T + c 2 ρ 2k+2   + 4 (1 − γ) 2 E init      Q Π θ s (·) − Proj F R,M Q Π θ s (·) 2 L 2 (ν θ ) (II)      . (D.12)
Term (II) measures the distance between Q Π θ s and the class F R,M . As discussed in Section D.1.1, the function class F R,M converges to F s,k R,∞ (defined in Assumption 5.1) as M → ∞. Consequently, term (II) decreases as the neural network gets wider. To quantitatively characterize the approximation error between F R,M and F s,k R,∞ , one needs the following lemma from Rahimi and Recht [46] and [6,Proposition 4.3]: 
Lemma D.3 Assume Assumption 5.1, we have E init      Q Π θ s (·) − Proj F R,M Q Π θ s (·) 2 L 2 (ν θ ) (II)      ≤ O R 2 d ζ∇ θ J( θ) (θ − θ) ≤ 0, ∀θ ∈ B.
(D.14)

Define the following mapping G from R M ×d ζ to itself:
G(θ) := η −1 · [Proj B (θ + η · ∇ θ J(θ)) − θ] . (D.15)
It is well-known that (D.14) holds if and only if G( θ) = 0 (Sra et al. [49]). Now denote ρ(t) := G(θ(t)), where θ(t) = {θ s (t)} s∈S is the actor parameter updated in Algorithm 2 in iteration t.

To show that Algorithm 2 converges to a stationary point, we focus on analyzing ρ(t) 2 .

Theorem D. 
E ρ(t) 2 2 ≤ 8τ 2 Σ 2 |S| B + 4 √ T actor E[J(θ(T actor + 1)) − J(θ(1))] + Q (T actor ). (D.16)
Here Q measures the error accumulated from the critic steps which is defined as
Q (T actor ) = 32τ DRd 1/2 ζs |S| (1 − γ)ηT actor · Tactor t=1 s∈S E Q s ( · ;ω s , t) − Q Π θ(t) s (·) L 2 (ν θ(t) ) + 16τ 2 D 2 |S| 2 (1 − γ) 2 T actor · Tactor t=1 s∈S E Q s ( · ;ω s , t) − Q Π θ(t) s (·) 2 L 2 (ν θ(t) )
,
(D.17)
where {Q s ( · ;ω s , t)} s∈S is the output of the critic update at step t in Algorithm 2. All expectations in (D. 16) and (D.17) are taken over all randomness in Algorithm 1 and Algorithm 2.

Proof. Proof of Theorem D.5 Let t ∈ [T actor ], we first lower bound the difference between the expected total rewards of Π θ(t+1) and Π θ(t) . By Assumption 5.7, ∇ θ J (θ) is L-Lipschitz continuous. Hence by Taylor's expansion,
J (θ(t + 1)) − J (θ(t)) ≥ η · ∇ θ J (θ(t)) δ(t) − L/2 · θ(t + 1) − θ(t) 2 2 , (D.18)
where δ(t) = (θ(t + 1) − θ(t)) /η. Meanwhile denote ξ s (t) = g s (θ(t))−E [ g s (θ(t))], where g s (θ(t)) is defined in (4.14) and the expectation is taken over σ θ(t) given {ω s } s∈S . Then
∇ θ J (θ(t)) δ(t) = s∈S ∇ θs J (θ(t)) δ s (t) = s∈S (∇ θs J (θ(t)) − E [ g s (θ(t))]) δ s (t) − ξ s (t) δ s (t) + g s (θ(t)) δ s (t) , (D.19)
where δ s (t) := (θ s (t + 1) − θ s (t)) /η. The first term in (D. 19) represents the error of estimating ∇ θs J (θ(t)) using
E [ g s (θ(t))] = 1 1 − γ E σ θ(t)   y∈N k s Q y µ(N k y ), h(N k y );ω y , t ∇ θs log Π θs (h(s) | µ(s))   .
To bound the first term, first notice that
E [ g s (θ(t))] = 1 1 − γ E σ θ(t)     y∈S Q y µ(N k y ), h(N k y );ω y , t   ∇ θs log Π θs (h(s) | µ(s))   .
This is because for all y ∈ N k s , Q y µ(N k y ), h(N k y );ω y is independent of s and consequently, we can verify that
E σ θ(t)     y ∈N k s Q y µ(N k (y)), h(N k (y));ω y , t   ∇ θs log Π θs (h(s) | µ(s))   = 0.
Therefore, following the similar computation in Lemma D.2, Cai et al. [6], we have
(∇ θs J (θ(t)) − E [ g s (θ(t))]) δ s (t) ≤ 4τ DRd 1/2 ζs (1 − γ)η s∈S Q s ( · ;ω s , t) − Q θ(t) s (·) L 2 (ν θ(t) )
. (D.20)

To bound the second term in (D. 19), we simply have
ξ s (t) δ s (t) ≤ ξ s (t) 2 2 + δ s (t) 2 2 . (D.21)
To handle the last term in (D.19), we have
g s (θ(t)) δ s (t) − δ s (t) 2 2 = η −1 · (η g s (θ(t)) − (θ s (t + 1) − θ s (t))) δ s =η −1 · θ s (t + 1/2) − Proj Bs (θ s (t + 1/2)) δ s (t) =η −2 · θ s (t + 1/2) − Proj Bs (θ s (t + 1/2)) Proj Bs (θ s (t + 1/2)) − θ s (t) ≥ 0 (D.22)
Here we write θ s (t) + η g s (θ(t)) as θ s (t + 1/2) to simplify the notation. The last inequality comes from the property of the projection onto a convex set. Therefore, combining (D. 19 
∇ θs J (θ(t)) δ s (t) ≥ − 4τ DRd 1/2 ζs (1 − γ)η s∈S Q s ( · ;ω s , t) − Q θ(t) s (·) L 2 (ν θ(t) ) + 1 2 δ s (t) 2 2 − ξ s (t) 2 2 .
Consequently,
∇ θ J (θ(t)) δ(t) ≥ − 4τ DRd 1/2 ζs (1 − γ)η |S| s∈S Q s ( · ;ω s , t) − Q Π θ(t) s (·) L 2 (ν θ(t) ) + 1 2 δ(t) 2 2 − ξ(t) 2 2 .
(D.23) Thus, by plugging (D.23) into (D.18) and by Assumption 5.5, we have
1 − L · η 2 E δ(t) 2 2 ≤ η −1 · E [J(θ(t + 1)) − J(θ(t))] + τ 2 Σ 2 |S| 2B + 4τ DRd 1/2 ζs |S| (1 − γ)η s∈S Q s ( · ;ω s , t) − Q Π θ(t) s (·) L 2 (ν θ(t) )
.


(D.24)

Here the expectation is taken over σ θ(t) given {ω s } s∈S . Now, in order to bridge the gap between δ(t) 2 in (D.24) and ρ(t) 2 = G(θ(t)) 2 in (D.15), we next will bound the difference δ(t) − ρ(t) 2 . We start with defining a local gradient mapping G s from R M ×d ζ to R M ×d ζs :
G s (θ) := η −1 · Proj Bs (θ s + η · ∇ θs J(θ)) − θ s . (D.25)
Since B s is an l ∞ -ball around the initialization, it is easy to verify that G s (θ) = (G(θ)) s . Therefore, we can further define ρ s (t) = G s (θ(t)) and the following decomposition holds:
δ(t) − ρ(t) 2 2 = s∈S δ s (t) − ρ s (t) 2 2 .
From the definitions of δ s (t) and ρ s (t), δ s (t) − ρ s (t) 2 = η −1 · Proj Bs (θ s + η · ∇ θs J(θ)) − θ s − Proj Bs (θ s + η · g s (θ)) + θ s 2 = η −1 · Proj Bs (θ s + η · ∇ θs J(θ)) − Proj Bs (θ s + η · g s (θ)) 2
≤ η −1 · θ s + η · ∇ θs J(θ) − θ s + η · g s (θ) 2 = ∇ θs J(θ) − g s (θ) 2
Following similar calculations in [6, Lemma D.3],
E ∇ θs J(θ) − g s (θ) 2 2 ≤ 2τ 2 Σ 2 B + 8τ 2 D 2 (1 − γ) 2 s∈S Q s ( · ;ω s , t) − Q Π θ(t) s (·) L 2 (ν θ(t) ) 2 ≤ 2τ 2 Σ 2 B + 8τ 2 D 2 |S| (1 − γ) 2 s∈S Q s ( · ;ω s , t) − Q Π θ(t) s (·) 2 L 2 (ν θ(t) )
.


(D.26)

The expectation is taken over σ θ(t) given {ω s } s∈S . Consequently,
E δ(t) − ρ(t) 2 2 ≤ 2τ 2 Σ 2 |S| B + 8τ 2 D 2 |S| 2 (1 − γ) 2 s∈S Q s ( · ;ω s , t) − Q Π θ(t) s (·) 2 L 2 (ν θ(t) )
. (D.27)

Set η = 1/ √ T actor and take (D.24) and (D.27), we obtain (D.16) from the following estimations:
min t∈[Tactor] E ρ(t) 2 2 ≤ 1 T actor · Tactor t=1 ρ(t) 2 2 ≤ 2 T actor · Tactor t=1 E δ(t) − ρ(t) 2 2 + E δ(t) 2 2 ≤ 2 T actor · Tactor t=1 E δ(t) − ρ(t) 2 2 + 2(1 − L · η)E δ(t) 2 2 ≤ 8τ 2 Σ 2 |S| B + 4 √ T actor E[J(θ(T actor + 1)) − J(θ(1))] + Q (T actor ),
where Q measures the error accumulated from the critic steps which is defined in (D.17), i.e.,
Q (T actor ) = 32τ DRd 1/2 ζs |S| (1 − γ)ηT actor · Tactor t=1 s∈S E Q s ( · ;ω s ) − Q Π θ(t) s (·) L 2 (ν θ(t) ) + 16τ 2 D 2 |S| 2 (1 − γ) 2 T actor · Tactor t=1 s∈S E Q s ( · ;ω s ) − Q Π θ(t) s (·) 2 L 2 (ν θ(t) )
.

Here the expectations in (D. 16) and (D.17) are taken over all randomness in Algorithm 1 and Algorithm 2. Q.E.D.


D.2.2 Bridging the gap between Stationarity and Optimality

Recall that σ θ in (4.2) denotes the state-action visitation measure under policy Π θ . Denoteσ θ as the state visitation measure under policy Π θ . Consequently, σ θ (µ)Π θ (h | µ) = σ θ (µ, h).

Following similar steps in the proof of [6,Theorem 4.8], one can characterize the global optimality of the obtained stationary point θ ∈ B as the following.

Lemma D.6 Let θ ∈ B be a stationary point of J(·) satisfying condition (D.14) and let θ * ∈ B be the global maximum point of J(·) in B. Then the following inequality holds: Proof. Proof of Lemma D.6 First recall that by (4.8), for any θ ∈ B,
∇ θ J( θ) (θ − θ) = s∈S ∇ θs J( θ) (θ s − θ s ) = τ 1 − γ s∈S E σ θ Q Π θ (µ, h) · Φ( θ, s, µ, h) (θ s − θ s ) ,
in which Φ(θ, s, µ, h) := φ θs (µ(s), h(s)) − E h(s) ∼Π θs s (·|µ(s)) [φ θs (µ(s), h (s))] is defined in (4.7). Since θ ∈ B is a stationary point of J(·), 
s∈S E σ θ Q Π θ (µ, h) · Φ( θ, s, µ, h) (θ s − θ s ) ≤ 0, ∀θ ∈ B. (D.29) Denote A Π θ (µ, h) := Q Π θ (µ, h) − V Π θ (µ)[A Π θ (µ, h)] = V Π θ (µ) − V Π θ (µ) = 0. Meanwhile, sup (µ,h)∈Ξ A Π θ (µ, h) ≤ 2 sup µ∈P N (S) V Π θ (µ) ≤ 2rmax 1−γ .
Given that E h∼Π θ (·|µ) [A Π θ (µ, h)] = 0 and E h∼Π θ (·|µ) [Φ( θ, s, µ, h)] = 0, we have for any s ∈ S, (1 − γ) · J(θ * ) − J( θ) = Eσ θ * A Π θ (µ, ·), Π θ * (· | µ) − Π θ (· | µ) .
E σ θ V Π θ (µ) · Φ( θ,
(D.33)

Combining (D.33) with (D.32), it holds that for any θ ∈ B, given an initialization θ s (0) ∈ R M ×d ζs , s ∈ S and b ∈ R M . F R,M (D.36) is a local linearization of the actor neural network. More specifically, term ( ) in (D.36) locally linearizes the decentralized actor neural network f (ζ s ; θ s ) (4.4) with respect to θ s . Any f 0 (ζ; θ) ∈ F R,M is a sum of |S| inner products between feature mapping φ θs(0) (·) (4.6) and parameter θ s : f 0 (ζ; θ) = s∈S φ θs(0) (ζ s ) · θ s . As the width of the neural network M → ∞, F R,M converges to F R,∞ (defined in Assumption 5.9). The approximation error between F R,M and F R,∞ is bounded in the following lemma.
(1 − γ) · J(θ * ) − J( θ) ≤ Eσ θ * A Π θ (µ, ·), Π θ * (· | µ) − Π θ (· | µ) − s∈S E σ θ A Π θ (ζ) · φ θs (ζ s ) (θ s − θ s ) = E σ θ A Π θ (µ, h) · dσ θ * dσ θ (µ, h) − dσ θ * dσ θ (µ) −
Lemma D.7 For any function f (ζ) ∈ F R,∞ defined in Assumption 5.9, we have
E init f (·) − Proj F R,M f (·) L 2 (σ θ ) ≤ O |S|Rd 1/2 ζs M 1/2 . (D.37)
Lemma D.7 follows from Rahimi and Recht [46] and [6,Proposition 4.3]. The factor |S| stems from the fact that F R,∞ can be decomposed into |S| independent reproducing kernel Hilbert spaces. With Lemma D.7, we are ready to establish an upper bound for the right-hand-side of (D.28) in the following proposition. , where F R,M is defined in (D.36). We denote Proj F R,M u θ (ζ) = s∈S φ θs(0) (ζ s ) · θ s ∈ F R,M for some θ ∈ B. Therefore, by Lemma D.7, the first term on the right-hand-side of (D. The following Lemma D.9 is a direct application of [53,Lemma E.2], which is used to bound the second term on the right-hand-side of (D.39).

Lemma D.9 It holds for any θ s , θ s ∈ B s = α s ∈ R M ×d ζs : α s − θ s (0) ∞ ≤ R/ √ M that E init φ θs (ζ s ) θ s − φ θs(0) (ζ s ) θ s L 2 (σ θ ) ≤ O  Proof. Proof of Theorem 5.10 Following similar calculations as in [53,Section H.3], we obtain that at iteration t ∈ [T actor ], ∇ θ J(θ(t)) (θ − θ(t)) ≤ 2(R + η · r max 1 − γ ) · ρ(t) 2 , ∀θ ∈ B.  


i (s s s, a a a) + γE s s s ∼P P P (s s s,a a a) [V (s s s )], (2.4)


s) (A) over all states by H N (µ) := {h : h(s) ∈ P N ·µ(s) (A) ∀ s ∈ S}. Here H N (µ) depends on µ and is a subset of H = {h : S → P(A)}.

Figure 1 :
1Left: MF-MARL problem (2.6)-(2.8). Right: Reformulation of team game (3.2)-(3.6).


feature mapping φ θs = [φ θs ] 1 , . . . , [φ θs ] M : R d ζs → R M ×d ζs may take the following form:

Algorithm 1
1Localized-Training-Decentralized-Execution Neural Temporal Difference 1: Input: Width of the neural network M , radius of the constraint set R, number of iterations T critic , policy Π θ , learning rate η critic , localization parameter k. 2: Initialize: For all m ∈ [M ] and s ∈ S, sample b m ∼ Unif({−1, 1}), [ω s (0)] m ∼ N 0, I d s = ω s (0). 3: for t = 0 to T critic − 2 do 4:

1 :
1Input: Width of the neural network M , radius of the constraint set R, number of iterations T actor and T critic , learning rate η actor and η critic , temperature parameter τ , batch size B, localization parameter k. 2: Initialize: For all m ∈ [M ] and s ∈ S, sample b m ∼ Unif({−1, 1}), [θ s (0)] m ∼ N 0, I d ζs /d ζs . 3: for t = 1 to T actor do 4: Define the policy Π θ (h | µ) := s∈S Π θs s (h(s) | µ(s)) = s∈S exp[τ · f ((µ(s), h(s)); θ s )] h (s)∈H N exp [τ · f ((µ(s), h (s)); θ s )]


12: end for 13: Output: {Π θ(t) } t∈[Tactor] .

Remark 5. 3
3Both Assumption 5.1 and Assumption 5.2 are similar to the standard assumptions in the analysis of single-agent neural actor-critic algorithms (Cai et al.[6], Liu et al.[38], Wang et al.[53], Cayci et al.[13]).In particular, Assumption 5.1 is a regularity condition for Q Π θ s in (Local Q-function). Here F s,k R,∞ is a subset of the reproducing kernel Hilbert space (RKHS) induced by the random feature1 v ζ k s > 0 · (ζ k s ) with v ∼ N (0, I d ζ k s /d ζ k s )up to the shift of Q s (ζ k s ; ω s (0)) (Rahimi and Recht[46]). This RKHS is dense in the space of continuous functions on any compact set (Micchelli et al.[41], Ji et al.[30]). (See also Section D.1.1 for details of the connection between F s,k R,∞ and the linearizations of two-layer neural networks (D.4)).Assumption 5.2 holds when σ θ and ν θ have uniformly upper bounded probability densities (Cai et al.[6]).

Theorem 5 . 4 (
54Convergence of Critic Update) Assume Assumptions 5.1 and 5.2. Set T critic = Ω(M ) and η critic = min{(1 − γ)/8, (T critic ) −1/2 } in Algorithm 1. Then Q s ( · ;ω s ) generated by Algorithm 1 satisfies

Theorem 5. 10
10Assume Assumptions 5.1 -5.9  . Set T critic = Ω(M ), η critic = min{(1 − γ)/8, (T critic ) −1/2 }, η actor = (T actor ) −1/2 , R = τ = 1, M = Ω (f (k)|A|) 5 (T actor ) 8 , γ ≤ (T actor ) −2/k , with f (k) := max s∈S |N k s | the size of the largest k-neighborhood in the graph (S, E). Then, the output {θ(t)} t∈[Tactor] of Algorithm 2 satisfies min t∈[Tactor] E [J(θ * ) − J(θ(t))] ≤ O |S| 1/2 B −1/2 + |S||A| 1/4 γ k/8 + (T actor ) −1/4 . (5.3)The error O(γ k/8 |S||A| 1/4 ) in Theorem 5.10, coming from the localized training, decays exponentially fast as k increases and is negligible with a careful choice of k. According to Theorem 5.10, Algorithm 2 converges at rate T −1/4 actor with sufficiently large width M and batch size B. Technically, {θ s (t)} s∈S in Algorithm 2 are updated in parallel and our analysis extends the single agent actor-critic in Cai et al.[6] to the multi-agent decentralized case.


s, µ t (N s ), a)π(s, µ t (s))(a) µ 0 = µ . We see (2.7)-(MF-MARL) is reformulated in an equivalent form of (A.2)-(A.3).


section presents the proof of convergence of the decentralized neural critic update. It consists of several steps. Section D.1.1 introduces necessary notations and definitions. Section D.1.2 proves that the critic update minimizes the projected mean-square Bellman error given a two-layer neural network. Section D.1.3 shows that the global minimizer of the projected mean-square Bellman error converges to the true team-decentralized Q-function as the width of hidden layer M → ∞.

D.1. 2
2Convergence to the Global Minimizer in F R,M The following lemma guarantees the existence and the uniqueness of the global minimizer of MSPBE that corresponds to the projection onto F R,M in (D.6). Lemma D.1 (Existence and Uniqueness of the Global Minimizer in F R,M ) For any b ∈ R M and ω(0) ∈ R M ×d ζ k s , there exists an ω * such that Q 0 ( · ; ω * ) ∈ F R,M is unique almost everywhere in F R,M and is the global minimizer of MSPBE that corresponds to the projection onto F R,M in (D.6).


lemma, Theorem 5.4 follows immediately by plugging (D.13) into (D.12), and setting c = rmax 1−γ , ρ = √ γ, T critic = Ω(M ) in (D.12). 2 Q.E.D. D.2 Proof of Theorem 5.10: Convergence of Actor Update The proof of Theorem 5.10 consists of two steps: the first step in Section D.2.1 shows that the actor update converges to a stationary point of J (4.1), and the second step in Section D.2.2 bridges the gap between the stationary point and the optimality. For the rest of this section, we use η to denote η actor and B s to denote B actor s := θ s ∈ R M ×d ζs : θ s − θ s (0) ∞ ≤ R/ √ M for ease of notation. Meanwhile, define B = s∈S B s , the product space of B s 's, which is a convex set in R M ×d ζ . D.2.1 Convergence to Stationary Point Definition D.4 A point θ ∈ B is called a stationary point of J(·) if it holds that


), (D.20), (D.21) and (D.22) suggests


θ (µ, h) := dσ θ * dσ θ (µ, h)− dσ θ * dσ θ (µ)+ s∈S φ θs (µ(s), h(s)) θ s , and dσ θ * dσ θ , dσ θ * dσ θare the Radon-Nikodym derivatives between the corresponding measures.

E
σ θ A Π θ (µ, h) · E h(s) ∼Πθs s (·|µ(s)) φ θs (µ(s), h (s)) = 0. (D.31) Combining (D.29) with (D.30) and (D.31), s∈S E σ θ A Π θ (µ, h) · φ θs (µ(s), h(s)) (θ s − θ s ) ≤ 0, ∀θ ∈ B. (D.32) Moreover, by the Performance Difference Lemma (Kakade and Langford [32]),

1
φ θs (µ(s), h(s)) (θ s − θ s ) . (D.34)Therefore,(1 − γ) · J(θ * ) − J( θ) ≤ 2r max 1 − γ inf θ∈B dσ θ * dσ θ (µ, h) − dσ θ * dσ θ (µ) − s∈S φ θs (µ(s), h(s)) (θ s − θ s ) L 2 (σ θ ) = 2r max 1 − γ inf θ∈B u θ (µ, h) − s∈S φ θs (µ(s), h(s)) θ s L 2 (σ θ ) , (D.35) where u θ (µ, h) := dσ θ * [θ s (0)] m ζ s > 0 [θ s ] m ζ s ( ) :θ s ∈ R M ×d ζs , θ s − θ s (0)

Proposition D. 8
8Under Assumption 5.9, let θ ∈ B be a stationary point of J(·) and let θ * ∈ B be the global maximum point of J(·) in B. Then the following inequality holds:(1 − γ) J(θ * ) − J( θ) Proof of Proposition D.8 First by the triangle inequality,inf θ∈B u θ (ζ) − s∈S φ θs (ζ s ) θ s L 2 (σ θ ) ≤ u θ (ζ) − Proj F R,M u θ (ζ) F R,M u θ (ζ) − s∈S φ θs (ζ s ) θ s L 2 (σ θ )


39) is bounded by (D.37):u θ (ζ) − s∈S φ θs(0) (ζ s ) · θ s L 2 (σ θ )


expectation is taken over random initialization.Taking θ = θ and θ = θ in Lemma D.9 gives us s∈S φ θs(0) (ζ s ) · θ s − φ θs (ζ s )


Now we are ready to establish Theorem 5.10.


-hand-side of (D.41) quantifies the deviation of θ(t) from a stationary point θ. Having (D.41) and following similar arguments for Lemma D.6 and Proposition D.8, we can show that (1 − γ) min t∈[Tactor] E [J(θ * ) − J(θ(t))] last term min t∈[Tactor] E[ ρ(t) 2 ] is bounded by (D.16) in Theorem D.5, while the term Q (T actor ) in (D.17) can be upper bounded by Theorem 5.4. Finally with the parameters stated in Theorem 5.10, the following statement holds by straightforward calculation: min t∈[Tactor] E [J(θ * ) − J(θ(t))] ≤ O |S| 1/2 B −1/2 + |S||A| 1/4 γ k/8 + (T actor ) −1/4 .


5 Assume Assumptions 5.5 -5.7. Set η = (T actor ) −1/2 and assume 1 − Lη ≥ 1/2, where L is the Lipschitz constant in Assumotion 5.7. Then the output {θ(t)} t∈[Tactor] of Algorithm 2 satisfiesmin 

t∈[Tactor] 




as the advantage function under policy Π θ . It holds from the definition that E h∼Π θ (·|µ)
Appendix
On the theory of policy gradient methods: Optimality, approximation, and distribution shift. A Agarwal, S M Kakade, J D Lee, G Mahajan, Journal of Machine Learning Research. 2298Agarwal A, Kakade SM, Lee JD, Mahajan G (2021) On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research 22(98):1- 76.

The entry and exit game in the electricity markets: a mean-field game approach. R Aïd, R Dumitrescu, P Tankov, Journal of Dynamics & Games. 84331Aïd R, Dumitrescu R, Tankov P (2021) The entry and exit game in the electricity markets: a mean-field game approach. Journal of Dynamics & Games 8(4):331.

A convergence theory for deep learning via over-parameterization. Z Allen-Zhu, Y Li, Z Song, International Conference on Machine Learning. PMLRAllen-Zhu Z, Li Y, Song Z (2019) A convergence theory for deep learning via over-parameterization. International Conference on Machine Learning, 242-252 (PMLR).

A finite time analysis of temporal difference learning with linear function approximation. J Bhandari, D Russo, R Singal, Conference on Learning Theory. PMLRBhandari J, Russo D, Singal R (2018) A finite time analysis of temporal difference learning with linear function approximation. Conference on Learning Theory, 1691-1692 (PMLR).

Solving n-player dynamic routing games with congestion: a mean-field approach. T Cabannes, M Lauriere, J Perolat, R Marinier, S Girgin, S Perrin, O Pietquin, A M Bayen, E Goubault, R Elie, arXiv:2110.11943arXiv preprintCabannes T, Lauriere M, Perolat J, Marinier R, Girgin S, Perrin S, Pietquin O, Bayen AM, Goubault E, Elie R (2021) Solving n-player dynamic routing games with congestion: a mean-field approach. arXiv preprint arXiv:2110.11943 .

Neural temporal-difference learning converges to global optima. Q Cai, Z Yang, J D Lee, Z Wang, Advances in Neural Information Processing Systems. 32Cai Q, Yang Z, Lee JD, Wang Z (2019) Neural temporal-difference learning converges to global optima. Advances in Neural Information Processing Systems, volume 32, 11315-11326.

Markov decision process routing games. D Calderone, S S Sastry, International Conference on Cyber-Physical Systems. IEEECalderone D, Sastry SS (2017) Markov decision process routing games. International Conference on Cyber-Physical Systems, 273-280 (IEEE).

An overview of recent progress in the study of distributed multi-agent coordination. Y Cao, W Yu, W Ren, G Chen, IEEE Transactions on Industrial informatics. 91Cao Y, Yu W, Ren W, Chen G (2012) An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics 9(1):427-438.

Mean-field games and systemic risk. R Carmona, J P Fouque, L H Sun, Communications in Mathematical Sciences. 134Carmona R, Fouque JP, Sun LH (2015) Mean-field games and systemic risk. Communications in Mathematical Sciences 13(4):911-933.

Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods. R Carmona, M Laurière, Z Tan, arXiv:1910.04295arXiv preprintCarmona R, Laurière M, Tan Z (2019) Linear-quadratic mean-field reinforcement learning: con- vergence of policy gradient methods. arXiv preprint arXiv:1910.04295 .

Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. R Carmona, M Laurière, Z Tan, arXiv:1910.12802arXiv preprintCarmona R, Laurière M, Tan Z (2019) Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802 .

Mean-field games with differing beliefs for algorithmic trading. P Casgrain, S Jaimungal, Mathematical Finance. 303Casgrain P, Jaimungal S (2020) Mean-field games with differing beliefs for algorithmic trading. Mathematical Finance 30(3):995-1034.

Sample complexity and overparameterization bounds for projection-free neural TD learning. S Cayci, S Satpathi, N He, R Srikant, arXiv:2103.01391arXiv preprintCayci S, Satpathi S, He N, Srikant R (2021) Sample complexity and overparameterization bounds for projection-free neural TD learning. arXiv preprint arXiv:2103.01391 .

Communication-efficient policy gradient methods for distributed reinforcement learning. T Chen, K Zhang, G B Giannakis, T Basar, IEEE Transactions on Control of Network Systems. Chen T, Zhang K, Giannakis GB, Basar T (2021) Communication-efficient policy gradient methods for distributed reinforcement learning. IEEE Transactions on Control of Network Systems .

Measure-valued Markov processes. École d'été de probabilités de Saint-Flour XXI-1991. D Dawson, SpringerDawson D (1993) Measure-valued Markov processes. École d'été de probabilités de Saint-Flour XXI-1991, 1-260 (Springer).

Multi-agent reinforcement learning for integrated network of adaptive traffic signal controllers (MARLIN-ATSC): Methodology and largescale application on downtown Toronto. S El-Tantawy, B Abdulhai, H Abdelgawad, IEEE Transactions on Intelligent Transportation Systems. 143El-Tantawy S, Abdulhai B, Abdelgawad H (2013) Multi-agent reinforcement learning for inte- grated network of adaptive traffic signal controllers (MARLIN-ATSC): Methodology and large- scale application on downtown Toronto. IEEE Transactions on Intelligent Transportation Systems 14(3):1140-1150.

Counterfactual multi-agent policy gradients. J Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson, AAAI Conference on Artificial Intelligence. 32Foerster J, Farquhar G, Afouras T, Nardelli N, Whiteson S (2018) Counterfactual multi-agent policy gradients. AAAI Conference on Artificial Intelligence, volume 32.

Single-timescale actor-critic provably finds globally optimal policy. Z Fu, Z Yang, Z Wang, International Conference on Learning Representations. Fu Z, Yang Z, Wang Z (2020) Single-timescale actor-critic provably finds globally optimal policy. International Conference on Learning Representations.

Correlation decay method for decision, optimization, and inference in largescale networks. D Gamarnik, Theory Driven by Influential Applications. INFORMSGamarnik D (2013) Correlation decay method for decision, optimization, and inference in large- scale networks. Theory Driven by Influential Applications, 108-121 (INFORMS).

Correlation decay in random decision networks. D Gamarnik, D A Goldberg, T Weber, Mathematics of Operations Research. 392Gamarnik D, Goldberg DA, Weber T (2014) Correlation decay in random decision networks. Mathematics of Operations Research 39(2):229-261.

A level-set approach to the control of state-constrained mckean-vlasov equations: application to renewable energy storage and portfolio selection. M Germain, H Pham, X Warin, arXiv:2112.11059arXiv preprintGermain M, Pham H, Warin X (2021) A level-set approach to the control of state-constrained mckean-vlasov equations: application to renewable energy storage and portfolio selection. arXiv preprint arXiv:2112.11059 .

Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, International Conference on Artificial Intelligence and Statistics. Glorot X, Bengio Y (2010) Understanding the difficulty of training deep feedforward neural net- works. International Conference on Artificial Intelligence and Statistics, 249-256.

H Gu, X Guo, X Wei, R Xu, arXiv:1911.07314Dynamic programming principles for learning MFCs. arXiv preprintGu H, Guo X, Wei X, Xu R (2019) Dynamic programming principles for learning MFCs. arXiv preprint arXiv:1911.07314 .

Mean-field controls with Q-learning for cooperative MARL: convergence and complexity analysis. H Gu, X Guo, X Wei, R Xu, SIAM Journal on Mathematics of Data Science. 34Gu H, Guo X, Wei X, Xu R (2021) Mean-field controls with Q-learning for cooperative MARL: convergence and complexity analysis. SIAM Journal on Mathematics of Data Science 3(4):1168- 1196.

Samod: Shared autonomous mobility-on-demand using decentralized reinforcement learning. M Guériau, I Dusparic, International Conference on Intelligent Transportation Systems. IEEEGuériau M, Dusparic I (2018) Samod: Shared autonomous mobility-on-demand using decentralized reinforcement learning. International Conference on Intelligent Transportation Systems, 1558-1563 (IEEE).

Learning mean-field games. X Guo, A Hu, R Xu, J Zhang, Advances in Neural Information Processing Systems. 32Guo X, Hu A, Xu R, Zhang J (2019) Learning mean-field games. Advances in Neural Information Processing Systems, volume 32, 4966-4976.

N-player and mean-field games in Itô-diffusion markets with competitive or homophilous interaction. R Hu, T Zariphopoulou, arXiv:2106.00581arXiv preprintHu R, Zariphopoulou T (2021) N-player and mean-field games in Itô-diffusion markets with com- petitive or homophilous interaction. arXiv preprint arXiv:2106.00581 .

Guided deep reinforcement learning for swarm systems. M Hüttenrauch, A Šošić, G Neumann, arXiv:1709.06011arXiv preprintHüttenrauch M, Šošić A, Neumann G (2017) Guided deep reinforcement learning for swarm sys- tems. arXiv preprint arXiv:1709.06011 .

Mean-field equilibria of dynamic auctions with learning. K Iyer, R Johari, M Sundararajan, Management Science. 6012Iyer K, Johari R, Sundararajan M (2014) Mean-field equilibria of dynamic auctions with learning. Management Science 60(12):2949-2970.

Neural tangent kernels, transportation mappings, and universal approximation. Z Ji, M Telgarsky, R Xian, International Conference on Learning Representations. Ji Z, Telgarsky M, Xian R (2020) Neural tangent kernels, transportation mappings, and universal approximation. International Conference on Learning Representations.

Real-time bidding with multi-agent reinforcement learning in display advertising. J Jin, C Song, H Li, K Gai, J Wang, W Zhang, ACM International Conference on Information and Knowledge Management. Jin J, Song C, Li H, Gai K, Wang J, Zhang W (2018) Real-time bidding with multi-agent re- inforcement learning in display advertising. ACM International Conference on Information and Knowledge Management, 2193-2201.

Approximately optimal approximate reinforcement learning. S Kakade, J Langford, International Conference on Machine Learning. PMLRKakade S, Langford J (2002) Approximately optimal approximate reinforcement learning. International Conference on Machine Learning, 267-274 (PMLR).

Actor-critic algorithms. V R Konda, J N Tsitsiklis, Advances in Neural Information Processing Systems. 12Konda VR, Tsitsiklis JN (2000) Actor-critic algorithms. Advances in Neural Information Processing Systems, volume 12, 1008-1014.

Mean-field and n-agent games for optimal investment under relative performance criteria. D Lacker, T Zariphopoulou, Mathematical Finance. 294Lacker D, Zariphopoulou T (2019) Mean-field and n-agent games for optimal investment under relative performance criteria. Mathematical Finance 29(4):1003-1038.

Efficient ridesharing order dispatching with mean-field multi-agent reinforcement learning. M Li, Z Qin, Y Jiao, Y Yang, J Wang, C Wang, G Wu, J Ye, The World Wide Web Conference. Li M, Qin Z, Jiao Y, Yang Y, Wang J, Wang C, Wu G, Ye J (2019) Efficient ridesharing order dispatching with mean-field multi-agent reinforcement learning. The World Wide Web Conference, 983-994.

Distributed reinforcement learning for decentralized linear quadratic control: A derivative-free policy optimization approach. Y Li, Y Tang, R Zhang, N Li, IEEE Transactions on Automatic Control. Li Y, Tang Y, Zhang R, Li N (2021) Distributed reinforcement learning for decentralized lin- ear quadratic control: A derivative-free policy optimization approach. IEEE Transactions on Automatic Control .

Multi-agent reinforcement learning in stochastic networked systems. Y Lin, G Qu, L Huang, A Wierman, Advances in Neural Information Processing Systems. 34Lin Y, Qu G, Huang L, Wierman A (2021) Multi-agent reinforcement learning in stochastic net- worked systems. Advances in Neural Information Processing Systems, volume 34.

Neural trust region/proximal policy optimization attains globally optimal policy. B Liu, Q Cai, Z Yang, Z Wang, Advances in Neural Information Processing Systems. 32Liu B, Cai Q, Yang Z, Wang Z (2019) Neural trust region/proximal policy optimization attains globally optimal policy. Advances in Neural Information Processing Systems, volume 32, 10565- 10576.

Off-policy policy gradient with stationary distribution correction. Y Liu, A Swaminathan, A Agarwal, E Brunskill, Conference on Uncertainty in Artificial Intelligence. PMLR115Liu Y, Swaminathan A, Agarwal A, Brunskill E (2019) Off-policy policy gradient with stationary distribution correction. Conference on Uncertainty in Artificial Intelligence, volume 115, 1180-1190 (PMLR).

Multi-agent actor-critic for mixed cooperative-competitive environments. R Lowe, Y I Wu, Tamar A Harb, J , Pieter Abbeel, O Mordatch, I , Advances in Neural Information Processing Systems. 30Lowe R, Wu YI, Tamar A, Harb J, Pieter Abbeel O, Mordatch I (2017) Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing Systems, volume 30, 6382-6393.

Universal kernels. C A Micchelli, Y Xu, H Zhang, Journal of Machine Learning Research. 712Micchelli CA, Xu Y, Zhang H (2006) Universal kernels. Journal of Machine Learning Research 7(12):2651-2667.

Mean-field markov decision processes with common noise and open-loop controls. M Motte, H Pham, arXiv:1912.07883arXiv preprintMotte M, Pham H (2019) Mean-field markov decision processes with common noise and open-loop controls. arXiv preprint arXiv:1912.07883 .

Policy gradient in lipschitz Markov decision processes. M Pirotta, M Restelli, L Bascetta, Machine Learning. 1002Pirotta M, Restelli M, Bascetta L (2015) Policy gradient in lipschitz Markov decision processes. Machine Learning 100(2):255-283.

Scalable reinforcement learning of localized policies for multi-agent networked systems. G Qu, A Wierman, N Li, Learning for Dynamics and Control. PMLRQu G, Wierman A, Li N (2020) Scalable reinforcement learning of localized policies for multi-agent networked systems. Learning for Dynamics and Control, 256-266 (PMLR).

Distributed optimization in sensor networks. M Rabbat, R Nowak, International Symposium on Information Processing in Sensor Networks. Rabbat M, Nowak R (2004) Distributed optimization in sensor networks. International Symposium on Information Processing in Sensor Networks, 20-27.

Uniform approximation of functions with random bases. A Rahimi, B Recht, Annual Allerton Conference on Communication, Control, and Computing. IEEERahimi A, Recht B (2008) Uniform approximation of functions with random bases. Annual Allerton Conference on Communication, Control, and Computing, 555-561 (IEEE).

QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. T Rashid, M Samvelyan, C Schroeder, G Farquhar, J Foerster, S Whiteson, International Conference on Machine Learning. PMLRRashid T, Samvelyan M, Schroeder C, Farquhar G, Foerster J, Whiteson S (2018) QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. International Conference on Machine Learning, 4295-4304 (PMLR).

Safe, multi-agent, reinforcement learning for autonomous driving. S Shalev-Shwartz, S Shammah, A Shashua, arXiv:1610.03295arXiv preprintShalev-Shwartz S, Shammah S, Shashua A (2016) Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295 .

S Sra, S Nowozin, S J Wright, Optimization for Machine Learning. MIT PressSra S, Nowozin S, Wright SJ (2012) Optimization for Machine Learning (MIT Press).

Value-decomposition networks for cooperative multi-agent learning based on team reward. P Sunehag, G Lever, A Gruslys, W M Czarnecki, V Zambaldi, M Jaderberg, M Lanctot, N Sonnerat, J Z Leibo, K Tuyls, G Thore, International Conference on Autonomous Agents and Multi-agent Systems. 3Sunehag P, Lever G, Gruslys A, Czarnecki WM, Zambaldi V, Jaderberg M, Lanctot M, Sonnerat N, Leibo JZ, Tuyls K, Thore G (2018) Value-decomposition networks for cooperative multi-agent learning based on team reward. International Conference on Autonomous Agents and Multi-agent Systems, volume 3, 2085-2087.

Policy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, Advances in Neural Information Processing Systems. 99Sutton RS, McAllester DA, Singh SP, Mansour Y (2000) Policy gradient methods for reinforce- ment learning with function approximation. Advances in Neural Information Processing Systems, volume 99, 1057-1063.

Calibration of shared equilibria in general sum partially observable markov games. N Vadori, S Ganesh, P Reddy, M Veloso, Advances in Neural Information Processing Systems. 33Vadori N, Ganesh S, Reddy P, Veloso M (2020) Calibration of shared equilibria in general sum par- tially observable markov games. Advances in Neural Information Processing Systems, volume 33, 14118-14128.

L Wang, Q Cai, Z Yang, Z Wang, Neural policy gradient methods: Global optimality and rates of convergence. International Conference on Learning Representations. Wang L, Cai Q, Yang Z, Wang Z (2020) Neural policy gradient methods: Global optimality and rates of convergence. International Conference on Learning Representations.

Sample efficient policy gradient methods with recursive variance reduction. P Xu, F Gao, Q Gu, International Conference on Learning Representations. Xu P, Gao F, Gu Q (2019) Sample efficient policy gradient methods with recursive variance re- duction. International Conference on Learning Representations.

An improved convergence analysis of stochastic variance-reduced policy gradient. P Xu, F Gao, Q Gu, Uncertainty in Artificial Intelligence. PMLRXu P, Gao F, Gu Q (2020) An improved convergence analysis of stochastic variance-reduced policy gradient. Uncertainty in Artificial Intelligence, 541-551 (PMLR).

Q-value path decomposition for deep multiagent reinforcement learning. Y Yang, J Hao, G Chen, H Tang, Y Chen, Y Hu, C Fan, Z Wei, International Conference on Machine Learning. PMLRYang Y, Hao J, Chen G, Tang H, Chen Y, Hu Y, Fan C, Wei Z (2020) Q-value path decomposition for deep multiagent reinforcement learning. International Conference on Machine Learning, 10706- 10715 (PMLR).

Multi-agent determinantal Q-learning. Y Yang, Y Wen, J Wang, L Chen, K Shao, D Mguni, W Zhang, International Conference on Machine Learning. PMLRYang Y, Wen Y, Wang J, Chen L, Shao K, Mguni D, Zhang W (2020) Multi-agent determinantal Q-learning. International Conference on Machine Learning, 10757-10766 (PMLR).

Toward packet routing with fully distributed multiagent deep reinforcement learning. X You, X Li, Y Xu, H Feng, J Zhao, H Yan, IEEE Transactions on Systems, Man, and Cybernetics: Systems. You X, Li X, Xu Y, Feng H, Zhao J, Yan H (2020) Toward packet routing with fully distributed multiagent deep reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics: Systems .

Global convergence of policy gradient methods to (almost) locally optimal policies. K Zhang, A Koppel, H Zhu, T Basar, SIAM Journal on Control and Optimization. 586Zhang K, Koppel A, Zhu H, Basar T (2020) Global convergence of policy gradient methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization 58(6):3586-3612.

Distributed learning of average belief over networks using sequential observations. K Zhang, Y Liu, J Liu, M Liu, T Başar, Automatica. 115108857Zhang K, Liu Y, Liu J, Liu M, Başar T (2020) Distributed learning of average belief over networks using sequential observations. Automatica 115:108857.

Networked multi-agent reinforcement learning in continuous spaces. K Zhang, Z Yang, T Basar, Conference on Decision and Control. IEEEZhang K, Yang Z, Basar T (2018) Networked multi-agent reinforcement learning in continuous spaces. Conference on Decision and Control, 2771-2776 (IEEE).

Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control. K Zhang, Z Yang, T Başar, SpringerZhang K, Yang Z, Başar T (2021) Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, 321-384 (Springer).

Fully decentralized multi-agent reinforcement learning with networked agents. K Zhang, Z Yang, H Liu, T Zhang, T Basar, International Conference on Machine Learning. PMLRZhang K, Yang Z, Liu H, Zhang T, Basar T (2018) Fully decentralized multi-agent reinforce- ment learning with networked agents. International Conference on Machine Learning, 5872-5881 (PMLR).

Finite-sample analysis for decentralized batch multi-agent reinforcement learning with networked agents. K Zhang, Z Yang, H Liu, T Zhang, T Basar, IEEE Transactions on Automatic Control. Zhang K, Yang Z, Liu H, Zhang T, Basar T (2021) Finite-sample analysis for decentralized batch multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic Control .

S Zheng, A Trott, S Srinivasa, N Naik, M Gruesbeck, D C Parkes, R Socher, arXiv:2004.13332The AI economist: Improving equality and productivity with AI-driven tax policies. arXiv preprintZheng S, Trott A, Srinivasa S, Naik N, Gruesbeck M, Parkes DC, Socher R (2020) The AI economist: Improving equality and productivity with AI-driven tax policies. arXiv preprint arXiv:2004.13332 .

Robust power management via learning and game design. Z Zhou, P Mertikopoulos, A L Moustakas, N Bambos, P Glynn, Operations Research. 691Zhou Z, Mertikopoulos P, Moustakas AL, Bambos N, Glynn P (2021) Robust power management via learning and game design. Operations Research 69(1):331-345.
