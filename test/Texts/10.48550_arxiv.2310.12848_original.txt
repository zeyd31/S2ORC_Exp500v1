
Neural Degradation Representation Learning for All-In-One Image Restoration


Mingde Yao 
Ruikang Xu 
Yuanshen Guan 
Jie Huang 
Member, IEEEZhiwei Xiong 
Neural Degradation Representation Learning for All-In-One Image Restoration
FBD39F80F504171D885DDC1C9F869B3FAll-in-one image restorationdegradation representationdenoisingderainingdehazingsuper-resolution
Existing methods have demonstrated effective performance on a single degradation type.In practical applications, however, the degradation is often unknown, and the mismatch between the model and the degradation will result in a severe performance drop.In this paper, we propose an all-in-one image restoration network that tackles multiple degradations.Due to the heterogeneous nature of different types of degradations, it is difficult to process multiple degradations in a single network.To this end, we propose to learn a neural degradation representation (NDR) that captures the underlying characteristics of various degradations.The learned NDR decomposes different types of degradations adaptively, similar to a neural dictionary that represents basic degradation components.Subsequently, we develop a degradation query module and a degradation injection module to effectively recognize and utilize the specific degradation based on NDR, enabling the all-in-one restoration ability for multiple degradations.Moreover, we propose a bidirectional optimization strategy to effectively drive NDR to learn the degradation representation by optimizing the degradation and restoration processes alternately.Comprehensive experiments on representative types of degradations (including noise, haze, rain, and downsampling) demonstrate the effectiveness and generalization capability of our method.

I. INTRODUCTION

Image restoration aims to recover high-resolution and clean images from degraded or low-quality images, thereby improving visual quality and benefiting downstream applications.To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1]- [5], deraining [6]- [10], dehazing [11]- [15], and super-resolution (SR) [16]- [20].These methods have shown great potential in addressing various image restoration tasks, which facilitates their application in practice.

However, existing restoration methods generally limit to one type of degradation and cannot be readily applied to multiple degradations, posing a challenging task in real-world applications.This is because the image degradation present in real-world scenarios is often unknown (as per [23], we use the term "unknown" to describe unspecific degradation, and it should not be confused with "unseen" degradation), and using a mismatched model for a specific type of degradation can result in a significant performance drop, as shown in Fig. 1.An alternative approach is to first assess the type of degradation and then select an appropriate model from a model library for restoration.However, this approach requires All authors are with University of Science and Technology of China, Hefei, 230026, China.E-mail: mdyao@mail.ustc.edu.cn,guanys@mail.ustc.edu.cn,guanys@mail.ustc.edu.cn,hj0117@mail.ustc.edu.cn,zwxiong@ustc.edu.cn.


Degradation-Specific
E 1 D 1 Trained in Degradation-1 ‚Ä¶ Backbone D 1 E 1 D 2 E 2 D ùëÅ E ùëÅ ‚Ä¶ Multi-head Network

Inputs

Methods Results Restormer [21] Fig. 1: Comparison between our method and other methods.[21], [22] fail to restore the clean image if the model mismatches the degradation.Our method can handle multiple degradations using a single network and produces more visually appealing results than the existing two-stage all-in-one model [23].


Two

a large model library and additional degradation assessment procedures, resulting in increased storage and computational overhead.Another approach, similar to the image signal processing (ISP) pipeline, is to sequentially apply all possible restoration models to restore the degraded image, but it still suffers from the problem of computational redundancy and error accumulation.Therefore, it is imperative to develop an all-in-one image restoration model that can handle various degradations using a single network.However, this is challenging due to the complex mapping relationships [24] between various degraded inputs and clean outputs, resulting in a network that is difficult to optimize.In addition, different degradations may possess distinct statistical properties that interfere with each other [23], [25], leading to a decline in performance.These difficulties hinder the effective handling of multiple degradations in a single network.

Recent studies have explored the feasibility of image restoration of multiple degradations, as shown in Fig. 1.IPT [22] employs multiple heads and tails with a shared body to process different types of degradations.However, the redundant heads and tails in IPT cause deployment challenges and it still relies on prior knowledge for head/tail selection.AirNet [23] proposes an all-in-one image restoration network arXiv:2310.12848v1[cs.CV] 19 Oct 2023 that utilizes contrastive learning to distinguish different degradations.By treating similar degradations as positive pairs and different degradations as negative pairs, it acquires distinct degradation representations for subsequent image restoration.However, AirNet requires two-stage training and additional training costs to support contrastive learning.

Different from previous methods [22], [23], in this paper, we propose to learn a neural degradation representation (NDR) that effectively captures the essential characteristics of various degradations.By leveraging NDR, our proposed all-in-one image restoration network, named NDR-Restore, can accurately identify and utilize the specific degradation of the input image, enabling adaptive restoration within a single network.Unlike existing representation learning methods [25], [26] that focus on capturing contents and texture details, NDR is specifically designed to learn the degradation representation.This not only allows NDR-Restore to leverage the crucial degradation information for image restoration, but also enables training NDR-Restore in an end-to-end way.

To build NDR-Restore, we propose a degradation query (DQ) module for degradation recognition and a degradation injection (DI) module for degradation utilization, allowing NDR-Restore to handle multiple degradations.Specifically, the DQ module is designed to query the degradation representation from NDR, which plays a key role in image restoration.This process yields a degradation tensor that contains degradation information of the input image.Then, the DI module injects the degradation tensor into the image feature for image restoration.In the DI module, we introduce low-rank feature modulation to project the degradation tensor and the image feature into the same space, facilitating the interaction between degradation information and image features.Finally, we seamlessly integrate the DQ and DI modules into a hierarchical encoder-decoder architecture to achieve robust restoration for different degradations.

For network optimization, we propose a bidirectional optimization strategy.Specifically, we introduce NDR-Degrad, an auxiliary degradation network that is jointly optimized with NDR-Restore.During the training process, NDR-Restore generates a clean image and queries the degradation tensor from NDR, while NDR-Degrad utilizes the queried degradation tensor to degrade a clean image.By optimizing NDR-Restore and NDR-Degrad alternately, we implicitly drive NDR to learn the degradation representation.This strategy relies on the rationale that, if NDR-Degrad could generate the specific degraded image, it indicates the queried degradation tensor effectively captures the degradation information from NDR.In other words, NDR indeed represents degradation.The NDR-Degrad is only utilized for training and once trained, we only need NDR-Restore for restoration.

We conducted comprehensive experiments on representative image restoration tasks, including denoising, deraining, dehazing, and SR.The experimental results demonstrate that NDR-Restore can effectively handle multiple degradations and outperforms existing all-in-one image restoration methods.Moreover, we evaluate our approach on real-captured images, revealing its great potential for practical applications in realworld scenarios.To summarize, our contributions are as fol-lows:

‚Ä¢ We propose a novel method for all-in-one image restoration, which provides a practical solution for handling multiple degradations using a single network.


II. RELATED WORK


A. Image Restoration

Image restoration is a fundamental task in computer vision, aiming to recover the original high-quality image from its degraded or corrupted version.In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], deraining [6]- [9], [28], [29], [29]- [33], dehazing [11], [34]- [36], and SR [37]- [40].For example, DnCNN [2] utilizes a deep convolutional architecture to learn the mapping between noisy and clean images, effectively suppressing the noise while preserving image details.IDT [33] utilizes a Transformerbased [41] architecture to capture the long-range dependencies for image deraining.N2V [42] proposes an self-supervised learning-based method with blind-spot convolution.Although the aforementioned methods have made significant progress, they are still confined to handling single degradations, which restricts their broad applicability.

All-in-one image restoration, which utilizes a single network to handle different restoration tasks, has emerged as a promising direction in computer vision.Early attempts [21], [43] utilize the same network architecture trained on different tasks with different parameters.However, training and deploying such networks for each task can still be inefficient despite the same architecture.In an effort to simplify the network, IPT [22] introduces the concept of body-sharing and uses different heads and tails for different restoration tasks.While this makes a step towards all-in-one image restoration, it still requires recognizing the degradation with additional degradation assessment and lacks versatility.More recently, AirNet [23] proposes an all-in-one image restoration network that processes different tasks by leveraging contrastive learning [44].ADMS [45] utilizes adaptive discriminative filters to handle different degradations and IDR [46] proposes a two-stage ingredients-oriented restoration network.Meanwhile, unified weather restoration methods [47]- [49] are also proposed.However, these methods are specifically designed for adverse weather conditions and require large efforts of two-stage training processes (or multi-head architectures).In contrast, our method adopts an end-to-end learning strategy to Fig. 2: Overview of our method.We construct NDR-Restore using a multi-scale architecture.NDR-Restore utilizes the DQ module to recognize degradation and leverages the DI module to facilitate the interaction between degradation information and image features.NDR captures the underlying characteristics of multiple degradations and is utilized in the DQ module to generate degradation.

train NDR, providing a more effective and versatile approach to all-in-one image restoration.


B. Degradation Representation Learning

The conventional image restoration pipeline uses a predefined model to represent degradation, e.g., Gaussian noise [2], [50] or motion blur [51], [52].The restoration model is trained to reverse the degradation process and restore the clean image based on the provided degradation model.In real-world scenarios, however, the degradation process is often unknown and complex, and explicit degradation representations become limited [53].Hence, it is crucial to find appropriate degradation representations for all-in-one image restoration.

To overcome this challenge, recent studies have suggested learning degradation representations using neural networks.For instance, DAN [54] uses an unfolding algorithm to learn a degradation kernel, capturing the intrinsic features of spatial resolution degradation.DASR [25] presents an unsupervised scheme to learn representations between various degradations in the feature space, enabling the network to adapt flexibly to different degradations in blind SR tasks.Similarly, AirNet [23] proposes a contrastive learning-based approach to learn a degradation representation for all-in-one image restoration.They train the network by treating similar degradations as positive pairs and different degradations as negative pairs, resulting in distinguishable degradation representations.In contrast to these methods, our proposed approach trains the NDR for all-in-one image restoration.NDR captures the intrinsic features of various degradation types, making it wellsuited for handling multiple degradation scenarios within a single network.


III. METHOD A. Overview

We propose neural degradation representation (NDR) to effectively represent the intrinsic features of different degradations, enabling a single network to handle multiple types of degradation.Specifically, NDR (D ‚àà R M √óN ) is a learnable tensor, where M is the feature dimension of degradation and N is the number of degradation types.Each vector in NDR serves as a type of learned degradation, which means it is not specific to the handcrafted degradation.NDR is independent of the context information and utilized in a query mechanism for degradation recognition (see Sec. III-B).

To leverage NDR, we construct an all-in-one image restoration network NDR-Restore, which is designed in a multi-scale encoder-decoder structure.In NDR-Restore, we introduce two novel modules for effective NDR utilization: the degradation query (DQ) module, which facilitates recognition of the image's degradation, and the degradation injection module, which allows for the utilization of the image's degradation into the restoration process, thereby enabling the all-in-one image restoration.

The pipeline of NDR-Restore is shown in Fig. 2. Given a degraded image x ‚àà R H√óW √ó3 , it is first fed into the encoder to extract the deep feature F ‚àà R H√óW √óC , where H, W, C represent the height, width, and channel shape, respectively.The feature F contains both the context and degradation information of the current image.Then, the DQ module takes the F and the degradation representation D to obtain the recognized degradation U ‚àà R H√óW √óC , which represents the degradation of the current image and is finegrained for each pixel.Subsequently, the DI module injects the recognized degradation U to the image feature F , enabling the degradation utilization with context information.Finally, the features are sent into the decoder to reconstruct a clean image ≈∑ ‚àà R H√óW √ó3 .It is worth noting that, we take the original scale inside the multi-scale architecture for illustration, while other scales perform in a similar way.We take transformbased attention mechanism [21] to implement the encoder and decoder, as shown in Fig. 3(a).

To optimize NDR, we propose a bidirectional optimization strategy by introducing a degradation network NDR-Degrad,


B. Degradation Query

To facilitate recognition of degradation in the current image, we propose the DQ module, which queries degradation from NDR.This module generates the recognized degradation U that represents the degradation of the degraded image.The DQ module consists of three main parts: feature mapping, affinity calculation, and degradation query, as illustrated in Fig. 2.

First, we map the input feature map F and neural degradation representation D to the same shape.We use a 1√ó1 convolution layer to rescale F 's channel dimension to match that of D, obtaining the rescaled feature
F ‚Ä≤ ‚àà R H√óW √óM .
Subsequently, we flatten F ‚Ä≤ into a two-dimensional tensor P ‚àà R HW √óM , where each pixel in P can match with a degradation in D.

Then, we calculate the affinity, which quantifies the relationship between each degradation in D and the mapped feature P .We utilize element-wise product to calculate the affinity of each pixel as
œÉ hw,n = M m=1 P hw,m ‚Ä¢ D m,n ,(1)
where œÉ hw,m represents the affinity between the hw-th pixel in P and the n-th degradation in D. It quantifies how much each degradation affects the pixels in the feature map P .By computing this affinity, we can determine the contribution of each degradation in D to the restoration process and effectively utilize it for accurate restoration.

For stabilizing the training process, we adopt a softmax operation to normalize œÉ hw,n , ensuring that the sum of contributions from various degradations is 1.This normalization step scales and stabilizes the individual degradation contributions, yielding an affinity matrix S ‚àà R HW √óN for each pixel.The calculation can be expressed as
S hw,n = e œÉ hw,n N n=1 e œÉ hw,n .(2)
Finally, we query the degradation information from D, thereby recognizing the degradation U ‚àà R H√óW √óCin for the current image.In this step, the affinity matrix S is leveraged to re-weight D and we obtain the recognized degradation U that captures the degradation information of the input image.To be detailed, we first utilize S to query the degradation from D as
U ‚Ä≤ hw,m = N n=1 S hw,n ‚Ä¢ D T n,m ,(3)
where U ‚Ä≤ hw,m represents the recognized degradation value at pixel location (h, w) with m-th dimension.To match the dimension number, we transpose D to D T in Eq. 3. Subsequently, we reshape U ‚Ä≤ ‚àà R HW √óM back to R H√óW √óM and employ a 1√ó1 convolution layer to map it to U ‚àà R H√óW √óC .Consequently, U becomes a degradation tensor where each pixel corresponds to a fine-grained recognized degradation.It can be effectively injected into the following image restoration process to adaptively remove degradation and restore clean images.

In Fig. 5, we present visualizations of the affinity matrix S and the recognized degradation U .For affinity matrix S, similar activations are observed along the same degradation


C. Degradation Injection

We propose the DI module to effectively utilize the recognized degradation U for adaptive degradation removal and clean image restoration.There are two challenges to be addressed.First, processing distinct degradation and image information is crucial, as they exist in different spaces.Second, since the degradation information is spatially related, the degradation utilization should consider the pixel-wise and content-aware nature of the image, thus ensuring the injected degradation aligns with the image content.

To overcome these challenges, the DI module maps the degradation and image features to the same space, allowing spatial utilization of the recognized degradation.We show the pipeline of the DI module in Fig. 2 and the details in Fig. 3(b).To achieve the mapping, we devise a Canonical Polyadic (CP)-Conv in the DI module, which can be written as
U cp = CP (U ), F cp = CP (F ).(4)
Here, CP represents the CP-Conv built on the CP decomposition [55], which effectively extracts the main characteristic and essential representation from the input data.Different from previous works [56] that directly apply CP decomposition to a single feature, we leverage it to simultaneously capture the essential representation of both image and degradation features, thereby mapping them into the same space.

The CP-Conv consists of a dimension projection and a Kronecker product [57], [58].We take U cp = CP (U ) in Eq. 4 as an example for simplicity, while F cp = CP (F ) can be easily inferred.The dimension projection utilizes three learnable projectors (p 1 , p 2 , and p 3 ) to map the input data to sub-features, yielding three 1-D features:
U 1 = p 1 (U ), U 2 = p 2 (U ), U 3 = p 3 (U ).(5)
Each projector comprises an average pooling layer, a 1√ó1 convolutional layer, and a sigmoid function.Here, ) , and K is set to be less than C, H, and W .These projections effectively integrate information along different dimensions, ensuring the low-rank property of the resulting features.Subsequently, the three features are conducted the Kronecker product as
U 1 ‚àà R K√ó(C√ó1√ó1) , U 2 ‚àà R K√ó(1√óH√ó1) , U 3 ‚àà R K√ó(1√ó1√óWU Kro = U 1 ‚äó U 2 ‚äó U 3 , U Kro ‚àà R K√óH√óW √óC ,(6)
where ‚äó denotes the Kronecker product that multiplies each element of a matrix with another complete matrix.Next, the U Kro is point-wise averaged along the K dimension to obtain U cp .Since U cp is derived from three rank-1 features, it has the low-rank property to lie in the closer space with the image feature F cp , resulting in the effective degradation injection.

To ensure the injected degradation aligns with the image content, we introduce the affine mapping as
F out = (F cp ‚äô U cp + U cp ) + F,(7)
where ‚äô denotes the element-wise production that spatially aligns the degradation with the image content.F out ‚àà R H√óW √óC represents the output of the DI module.In Fig. 6, we show the input image feature F and the corresponding residual map, which demonstrate the differences before and after the degradation injection.


D. Bidirectional Optimization

As aforementioned, NDR plays a crucial role in representing degradations and is a key component in NDR-Restore.However, NDR is merely a set of learnable parameters and training NDR-Restore in an end-to-end manner alone may not provide NDR with a clear physical interpretation.Therefore, we propose a bidirectional training strategy that constrains the degradation and restoration processes to drive the NDR to represent degradation.

To this end, we introduce an auxiliary degradation network NDR-Degrad.Unlike the restoration network NDR-Restore, NDR-Degrad generates degraded images from clean images using the given degradation.NDR-Degrad consists of three components: an encoder, a DI module (see Sec. III-C), and a decoder.The encoder and decoder construct the backbone of NDR-Degrad, and the DI module injects the degradation U into the extracted features, where U is given by the DQ module from NDR-Restore.

As shown in Fig. 4, the bidirectional training strategy optimizes both NDR-Restore and NDR-Degrad.For NDR-Restore, we input the degraded image x to obtain the restored image y ‚Ä≤ and the recognized degradation U .For NDR-Degrad, we input the clean image y and U to generate a degraded image x ‚Ä≤ .We bidirectionally optimize the two networks using the following loss function
L = L restore + L degrad = ‚à•x ‚àí x ‚Ä≤ ‚à• 2 + Œª‚à•y ‚àí y ‚Ä≤ ‚à• 2 , (8)
where Œª is a scaling factor.

The rationale behind this strategy is that if NDR-Degrad could generate a specific degraded image conditioned on the degradation tensor U , which is queried from NDR, it signifies that the queried tensor U effectively captures the degradation information, thereby validating NDR as an effective representation of the degradation.Consequently, during the bidirectional optimization process, NDR is implicitly driven to learn degradation representations.During the inference process, we only require NDR-Restore for all-in-one image restoration without the auxiliary network NDR-Degrad.


IV. EXPERIMENTS

We conduct experiments on two degradation settings, i.e., single degradation and multiple degradations.It is worth noting that, we do NOT aim to achieve state-of-the-art (SOTA) performance on single degradation since our focus is not the architecture/algorithm design on a specific task.Despite this, we report experimental results on the single degradation to demonstrate: 1) our method can work well on the single degradation, and 2) we provide an anchor to better analyze the performance on multiple degradations.


A. Experiments on Single Degradation a) Experimental settings:

We conduct experiments on four types of degradations, including noise, rain, haze, and spatial downsampling, where corresponding restoration tasks are denoising, deraining, dehazing, and SR.For denoising, we take the widely-used BSD400 [59] and WEB [67] datasets as training sets, and the BSD68 [59] and Urban100 [60] datasets as testing sets.To be detailed, BSD400 [59], WED [67], BSD68 [59], and Urban100 [60]  We select several recent methods as a baseline for comparison.For denoising, we compare our methods with CBM3D [1], DnCNN [2], IRCNN [3], FFDNet [4], and BRDNet [5].For deraining, we compare our methods with DIDMDN [6], UMRL [7], SIRR [8], MSPFN [9], and LPNet [10].For dehazing, we compare our methods with DehazeNet [11], MSCNN [12], AODNet [13], EPDN [14], FDGAN [15], and DehazeFormer [64].For SR, we compare our method with SR methods including EDSR [17], RCAN [37], HAN+ [19], SwinIR [68], HAT [39], and Restormer [21].We also compare with image restoration methods including AirNet [23], MPRNet [61], and Restormer [21].

All the methods are trained and tested using the same training and testing sets.We trained our model using the Adam optimizer with a learning rate of 1e-4.The batch size is set to 4 and the images are cropped to size of 128 √ó 128.We use a weight decay of 1e-4 and a momentum of 0.9.All the experiments are conducted on a single NVIDIA GTX 3090Ti GPU in PyTorch.We follow the same evaluation metrics as in previous works [23], including Peak Signal-to-Noise Ratio (PSNR) and structural similarity index measure (SSIM).

b) Quantitative results.: We perform a comprehensive quantitative comparison of our method against baseline methods on various image restoration tasks.For the denoising task, as shown in Table I, our method consistently outperforms existing methods on different datasets and at various noise levels.Notably, recent image restoration methods do not specifically tailor their network architecture for denoising but exhibit better     performance than previous denoising methods.For the deraining task, Table II shows the numerical results, demonstrating the robustness of our method in effectively removing rain streaks from rainy images.We also show the dehazing and SR results in Table III and Table IV, respectively.These quantitative results further validate the excellent performance of our method in handling single types of degradation.All these results demonstrate the effectiveness of our method on representative restoration tasks, while the SOTA performance on single degradation is not our goal.


B. Experiments on Multiple Degradations 1) Experimental settings:

To validate the effectiveness of our method, we conduct experiments on multiple types of image degradations, i.e., all-in-one image restoration.During the training phase, we mix datasets containing different types of degradations, allowing the network to learn from a variety of degradations.Subsequently, we evaluate the network's performance on multiple "unknown" degradations.It is worth noting that our definition of the "unknown" degradation aligns with [23] which refers to the unspecific degradation that is not explicitly recognized, rather than degradations that are unseen during the training phase.

We use four mixing configurations including denois-ing+deraining, denoising+dehazing, deraining+dehazing, and denoising+deraining+dehazing for training and evaluation.We use the same training datasets in Sec.IV-A and mix them together.The datasets are resampled to balance the training data.We compare our method with various image restoration techniques, including NAFNet [69], MPRNet [61], Restormer [21], and AirNet [23].All the baselines are trained for different configurations.We also conduct experiments with scale changes to consider the SR task.PSNR and SSIM are chosen as evaluation metrics.


Input

Ground Truth AirNet Restormer MPRNet Ours NAFNet 2) Quantitative results.: We quantitatively evaluate the methods on multiple degradations at the original scale, and the results are shown in Table V.It can be seen that, our method outperforms the baseline methods across various configurations.Notably, in the denoise+dehaze+derain setting, our dehazing performance shows an improvement of over 0.7dB compared with AirNet [23], while the deraining performance also gains approximately 0.5dB over AirNet.Similarly, for other configurations, significant performance gains are achieved over the baseline methods.An observation is that AirNet seems sensitive to the mixing of different degradations.In the denoise+derain configuration, it achieves a 4dB perfor-mance change (38.31 vs. 33.61) in the deraining task compared with AirNet's result in Table II.In the denoise+dehaze+derain configurations, the dehazing results also have a performance gap of approximately 4dB (27.94 vs. 23.18)compared with the results in Table III.This phenomenon might indicate potential instability in handling various degradations with AirNet.In contrast, our method exhibits more stable performance in handling multiple degradations under different configurations, which demonstrates that our proposed method effectively copes with various degradation types.

We further evaluate our method on multiple degradations with the SR task, and the results are shown in Table VI.It is  VI show that our method outperforms the baseline methods across different degradations with the SR task.This indicates the effectiveness and versatility of our proposed approach in handling complex real-world image restoration tasks.

3) Qualitative results.: We compare the visual results in Fig. 7 to demonstrate the effectiveness of our method across different restoration tasks.In each sub-figure, we provide visual comparisons between our approach and several baseline methods for denoising, deraining, and dehazing tasks, where the models are trained under the configuration of de-noise+derain+dehaze.For denoising, our method demonstrates superior noise reduction capabilities compared to the baseline methods.As seen in the first block of Fig. 7, our method outperforms DnCNN and FFDNet, preserving finer structures and textures, making it visually pleasing.In the deraining task, our method effectively removes rain streaks while preserving essential scene details.Fig. 7's second block shows that our method yields clearer derained images compared to AirNet.Notably, in the regions of the horse, our method successfully removes rain streaks, resulting in a more natural and visually appealing appearance.For the dehazing task (see the third block in Fig. 7), our method excels in enhancing visibility and contrast, which reduces haze and improves image sharpness and color.Notably, the distant objects become more recognizable and vivid in the dehazed image than baseline methods.The visual results demonstrate the superiority of our method in each individual task compared to the baseline method, proving the effectiveness of our method for all-in-one image restoration.

C. Experiments on Real-captured Images a) Experimental settings: To further demonstrate the effectiveness of our method, we evaluate our method on real captured images.For noisy images, we conduct experiments using the SIDD [72] dataset, which contains 200 images captured with smartphones.For rainy images, we use the practical subset in JORDER [73] for inference.This subset contains 15 real-world rainy images without ground truth.For hazy images, we use the RTTS subset in RESIDE [74] to conduct experiments, where the subset contains 4322 hazy images collected from the Internet.We compare our method with AirNet [23] and Restormer [21].All models are trained under the same configuration, i.e., noise+rain+haze, as described in Sec IV-B.

b) Qualitative results: We show the visual results in Fig. 9.It can be seen that the baseline methods tend to generate artifacts and fail to effectively remove noise, rain streaks, and haze from the images.In contrast, our proposed method demonstrates superior performance on all three tasks.In the denoising task, our method effectively reduces noise while preserving image details and textures, resulting in cleaner and visually appealing denoised images.In the deraining task, our method removes rain streaks and restores clear visibility.In the dehazing task, our method shows remarkable capabilities in removing haze.It effectively restores clear and visible scenes, recovering fine details obscured by haze in the input images.This proves the robustness and effectiveness of our approach across various degradations.Moreover, there is an interesting observation that the earlier checkpoint has a better generalization ability on real-world images, which could be further explored.c) Quantatitive results: To further demonstrate the effectiveness of our method in real-captured images, we calculate non-reference image quality assessment metrics (DBCNN [70] and MUSIQ [71]) and show them in Table VII.It can be seen that, our method has better performance than other baseline methods.


Input

Ground Truth AirNet Restormer MPRNet Ours NAFNet  Fig. 9: Qualitative results of different methods on real-captured images.It can be seen that our method has better performance than other methods.However, there is still space for performance improvement due to the large gap between real and simulated data.

V. ANALYSIS A. Ablation on NDR NDR plays a crucial role in capturing essential degradation characteristics, and its size might influence restoration performance.We explore the influence of NDR's size, i.e., feature dimensions (M ) and degradation types (N ), on restoration performance.We follow the denoise+derain+dehaze configuration to train the model and test it on the BSD68 dataset.As shown in Table VIII, when we remove NDR (-), the restoration performance drops significantly, demonstrating its importance in the network.As we increase the size of NDR by expanding M and N , we observe improved restoration performance.For example, when we increase the feature dimension from 32 to 128 and the number of degradation types from 8 to 16, the PSNR and SSIM scores show notable improvements, demonstrating the effectiveness of larger NDR sizes.

While increasing size improved restoration performance, further growing NDR did not produce substantial gains, which is the marginal effect.Therefore, we select (128, 32) as the NDR size since it achieves a balance between representation capacity and size.


B. Ablation on the Modules

In this section, we conduct ablation experiments to evaluate the impact of our proposed DQ and DI modules.Specifically, we compare our full model by replacing the DQ and DI modules with vanilla convolution.Moreover, we conduct ablation on the CP-Conv by replacing it with concatenation & convolution.Table IX shows the quantitative results of the ablation study, in terms of PSNR and SSIM.We follow the denoise+derain+dehaze configuration to train models and test   them on BSD68 [59] (œÉ = 25), Rain100L [32], and OTSoutdoor [63].

We observe that removing the DQ module leads to a decrease in PSNR and SSIM values, indicating its effectiveness in recognizing degradation information.Similarly, excluding the DI module also results in a notable drop, showing that the DI module plays a crucial role in utilizing the degradation information.Moreover, when CP-Conv is removed, the PSNR and the SSIM drops, indicating the necessity of CP-Conv.


C. Training Strategy

We demonstrate the effectiveness of our proposed training strategy through an ablation study.This study is performed on the noise+rain+haze configuration, as detailed in Sec IV-B, and the results are presented in Table X.The models trained without our proposed training strategy, referred to as "w/o strategy", consistently show lower performance compared to the models with our proposed training strategy.Notably, the denoising model achieves a higher PSNR, improving from 31.07 to 31.36, when trained with our strategy.Similarly, in the deraining and dehazing tasks, our training strategy leads to notable performance gains.These results validate the significance of our training strategy in enhancing restoration performance across various degradation types.

Moreover, we visualize the outputs of NDR-Degrad to further emphasize the effectiveness of NDR-Degrad, which can effectively generate degraded images.As shown in Fig. 10, the output image, denoted as x, has a similar degradation as the original degraded image x, indicating that NDR-Degrad is able to degrade the image.These visualizations provide clear evidence of NDR-Degrad's efficacy, which drives NDR to learn appropriate degradation representations.


D. Runtime and Parameter

We present the parameter count and runtime of our models here.For NDR-Restore, it takes only 0.25 seconds to restore a 512x512 image on a single GTX 3090Ti GPU, with 28.4M parameters.For NDR-Degrad, we adopt a lighter architecture with 10.70M parameters to assist the NDR learning.Notably, our NDR module consists of just 0.004M parameters, while the DQ and DI modules contribute 2.38 million parameters, which incurs relatively small overhead compared to the encoder and decoder (approximately 26.1M parameters).This exemplifies the lightweight nature of our core design, demonstrating its potential for integration into existing network architectures.


VI. CONCLUSIONS AND FUTURE WORKS

In this paper, we propose NDR-Restore, an all-in-one image restoration method that can process multiple types of degradations in a single network.The key idea is to learn the NDR that effectively captures essential degradation characteristics.To leverage NDR, we propose two novel modules, the DQ module and the DI module, which effectively recognize and utilize image degradations, respectively.To drive NDR to represent degradations, we devise a bidirectional optimization strategy, where an auxiliary degradation network NDR-Degrad is jointly optimized with NDR-Restore.Experimental results demonstrated the superiority of NDR-Restore over existing methods in denoising, deraining, dehazing, and SR tasks.

Fig. 3 :Fig. 4 :
34
Fig. 3: Network details.(a) The implementation of the encoder and decoder in NDR-Restore.It takes transformer-based attention mechanism [21] to extract shallow and deep features.(b) The details of CP-Conv, referred to Eq. 5 & 6.


Fig. 5 :
5
Fig. 5: Visualizations of NDR D, affinity matrices S, and recognized degradations U .Please note red dashed rectangles in affinity matrices.We can observe distinguishing activation of different degradations and similar activation of the same degradation, which demonstrates the accurate degradation recognition of DQ module and the effective degradation representation of NDR.type.This observation highlights the effectiveness of NDR in capturing the characteristic features of various degradations, while the DQ module successfully establishes corresponding relationships between NDR and the degraded image feature F .Moreover, the visualization of U reveals the spatial alignment of degradation patterns with the image pixels, which demonstrates the DQ module can accurately recognize and represent the specific degradation of the current image.


Fig. 6 :
6
Fig. 6: Features processed by the DI module.The residual features (before and after the DI module) only contain the degradation information, which demonstrates the effective degradation removal of the DI module.




datasets consist of 400, 4,744, 68, and 100 clean natural images.Following[2]-[5],[23], Gaussian noise is added with the standard deviation values of 15, 25, and 50 on the clean images.For deraining, we use the Rain100L[32]  dataset for training and testing.The training set contains 200 synthetic rainy images and their clean counterparts, and the testing set contains 100 rainyclean image pairs.For dehazing, we use the RESIDE [63] dataset, which consists of the Outdoor Training Set (OTS) and the Synthetic Objective Testing Set (SOTS) for training and testing, respectively.The OTS contains 72,135 outdoor hazyclean image pairs and the SOTS contains 500 outdoor hazyclean image pairs.For SR, we use the DIV2K [65] dataset to generate low-resolution images with scaling factors of 2, where the first 750 images are used for training and the rest 50 images are used for testing.The corresponding high-resolution images are used as the ground truth.


Fig. 7 :
7
Fig. 7: Qualitative comparisons of all-in-one image restoration.First block: Denoising on the Urban100 [60] dataset (œÉ = 25).Middle block: Deraining on the Rain100L [32] dataset.Last block: Dehazing on the SOTS-outdoor [63] dataset.


Fig. 8 :
8
Fig. 8: Qualitative comparisons of all-in-one image restoration with 2√ó SR.First block: Denoising on the Urban100 [60] dataset (œÉ = 25).Middle block: Deraining on the Rain100L [32] dataset.Last block: Dehazing on the SOTS-outdoor [63] dataset.


Fig. 10 :
10
Fig. 10: Visualization of NDR-Degrad's outputs.The output x exhibits similar degradations as the original degraded image x, demonstrating the effectiveness of NDR-Degrad in generating degraded images.PSNR and SSIM values are provided to further emphasize the high similarity between x and x.


TABLE I :
I
[60]titative results of denoising on the BSD68[59]and Urban100[60]datasets.
MethodœÉ=15BSD68 [59] œÉ=25œÉ=50œÉ=15Urban100 [60] œÉ=25œÉ=50CBM3D [1]33.50/0.9215 30.69/0.8672 27.36/0.762633.93/0.9408 31.36/0.9092 27.93/0.8404DnCNN [2]33.89/0.9290 31.23/0.8830 27.92/0.789632.98/0.9314 30.81/0.9015 27.59/0.8331IRCNN [3]33.87/0.9285 31.18/0.8824 27.88/0.789827.59/0.8331 31.20/0.9088 27.70/0.8396FFDNet [4]33.87/0.9290 31.21/0.8821 27.96/0.788733.83/0.9418 31.40/0.9120 28.05/0.8476BRDNet [5]34.10/0.9291 31.43/0.8847 28.16/0.794234.42/0.9462 31.99/0.9194 28.56/0.8577AirNet [23]34.14/0.9356 31.48/0.8928 28.23/0.805734.40/0.9487 32.10/0.9240 28.88/0.8702MPRNet [61]34.09/0.9313 31.38/0.8856 28.07/0.796734.40/0.9463 31.91/0.9166 28.52/0.8576Restormer [21]34.24/0.9337 31.57/0.8898 28.26/0.802634.71/0.9490 32.30/0.923129.02/0.8685Ours34.30/0.9346 31.65/0.8916 28.38/0.806534.80/0.9502 32.48/0.9263 29.31/0.8774

TABLE II :
II
Quantitative results of image deraining on the Rain100L[32]dataset.
MetricsDIDMDN [6] UMRL [7] SIRR [8] MSPFN [9] LPNet [10] AirNet [23] DRT [62] Restormer [21]OursPSNR23.7932.3932.3733.5033.6134.9037.6538.0538.33SSIM0.77310.92100.92580.94800.95830.96600.97500.97980.9839

TABLE III :
III
[63]titative results of dehazing on the SOTS-outdoor[63]dataset.
MetricsMSCNN [12] AODNet [13] EPDN [14] FDGAN [15] AirNet [23] DehazeFormer [64] Restormer [21]OursPSNR22.0620.2922.5723.1523.1830.8731.4731.96SSIM0.90780.87650.86300.92070.90000.97550.97850.9804

TABLE IV :
IV
[65]titative results of image SR on the DIV2K[65]dataset.
MetricsEDSR [17] CSNLN [66] RCAN [37] HAN+ [19] AirNet [23] Restormer [21]OursPSNR33.1533.3533.4333.4633.5233.5733.64SSIM0.92430.92740.92750.92760.92790.92990.9301

TABLE V :
V
Quantitative results for all-in-one image restoration.
Training datasetsMethodsNoise BSD68 (œÉ=15) BSD68 (œÉ=25) BSD68 (œÉ=50)Rain Rain100LHaze SOTSAverage

TABLE VI :
VI
Quantitative results for all-in-one image restoration with 2√ó SR.
Training datasetsMethodsNoise BSD68 (œÉ=15) BSD68 (œÉ=25) BSD68 (œÉ=50)Rain Rain100LHaze SOTSAverageworth noting that handling SR tasks with varying degradationsposes additional challenges as the downsampling degradationis considered. The results in Table

TABLE VII :
VII
[71]titative results on real-captured images in terms of non-reference metrics (DBCNN[70]and MUSIQ[71]).Higher values indicate better performance.
NoiseRainHazeRestormer [21] 27.97/30.58 51.62/60.35 42.66/53.65AirNet [23]26.97/29.29 52.37/60.42 43.12/53.57Ours35.04/31.97 52.56/60.58 43.77/54.24

TABLE VIII :
VIII
Abalation study on NDR.M is the feature dimension of degradation and N is the number of degradation types."-" means the network without NDR.
size (M, N )NoiseRainHaze-30.82/0.8762 34.82/0.9611 27.83/0.9587(32, 8)31.05/0.8840 35.27/0.9677 28.43/0.9603(64, 32)31.25/0.8862 35.33/0.9679 28.47/0.9602(128, 16)31.30/0.8872 35.37/0.9683 28.50/0.9609(128, 32)31.36/0.8873 35.42/0.9685 28.64/0.9616NoiseRainHazeInputRestormerAirNetOurs

TABLE IX :
IX
Abalation study on the proposed modules.DQ 30.71/0.873135.18/0.966128.15/0.9577w/o DI 30.75/0.821335.26/0.967327.99/0.9565w/o CP 31.07/0.884235.35/0.967728.34/0.9594Ours 31.36/0.887335.42/0.968528.64/0.9616
NoiseRainHazew/o

TABLE X :
X
Abalation study on the training strategy.strategy 31.07/0.885435.20/0.966228.42/0.9581Ours 31.36/0.887335.42/0.968528.64/0.9616
NoiseRainHazew/o NoiseRainHazeùë•‡∑ú ùë•(50.67/0.9997)
(52.76/0.9994)(52.81/0.9998)


. Noise+haze Nafnet, 69

. Rain+haze Nafnet, 69

. Mprnet, 43

. Airnet, 23

. Restormer, 21

. Noise+ Rain+haze, Nafnet , 69

. Noise+haze Nafnet, 69

. Rain+haze Nafnet, 69

. Mprnet, 43

. Airnet, 23

. Restormer, 21

. Noise+ Rain+haze, Nafnet , 69

Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminance-chrominance space. K Dabov, A Foi, V Katkovnik, K Egiazarian, 2007 IEEE International Conference on Image Processing. IEEE20071313

Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, IEEE transactions on image processing. 2672017

Learning deep cnn denoiser prior for image restoration. K Zhang, W Zuo, S Gu, L Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017

Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. K Zhang, W Zuo, L Zhang, IEEE Transactions on Image Processing. 2792018

Image denoising using deep cnn with batch renormalization. C Tian, Y Xu, W Zuo, Neural Networks. 1212020

Density-aware single image de-raining using a multi-stream dense network. H Zhang, V M Patel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018

Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining. R Yasarla, V M Patel, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019

Semi-supervised transfer learning for image rain removal. W Wei, D Meng, Q Zhao, Z Xu, Y Wu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019

Multi-scale progressive fusion network for single image deraining. K Jiang, Z Wang, P Yi, C Chen, B Huang, Y Luo, J Ma, J Jiang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020

Lightweight pyramid networks for image deraining. X Fu, B Liang, Y Huang, X Ding, J Paisley, IEEE transactions on neural networks and learning systems. 201931

Dehazenet: An end-to-end system for single image haze removal. B Cai, X Xu, K Jia, C Qing, D Tao, IEEE Transactions on Image Processing. 25112016

Single image dehazing via multi-scale convolutional neural networks. W Ren, S Liu, H Zhang, J Pan, X Cao, M.-H Yang, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part II 14

Aod-net: All-in-one dehazing network. B Li, X Peng, Z Wang, J Xu, D Feng, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017

Enhanced pix2pix dehazing network. Y Qu, Y Chen, J Huang, Y Xie, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019

Fd-gan: Generative adversarial networks with fusion-discriminator for single image dehazing. Y Dong, Y Liu, H Zhang, S Chen, Y Qiao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034

Image super-resolution using deep convolutional networks. C Dong, C C Loy, K He, X Tang, IEEE transactions on pattern analysis and machine intelligence. 201538

Enhanced deep residual networks for single image super-resolution. B Lim, S Son, H Kim, S Nah, K Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshops2017

Zero-shot dual-lens super-resolution. R Xu, M Yao, Z Xiong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023

Single image super-resolution via a holistic attention network. B Niu, W Wen, W Ren, X Zhang, L Yang, S Wang, K Zhang, X Cao, H Shen, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XII 16

Accurate image super-resolution using very deep convolutional networks. J Kim, J K Lee, K M Lee, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016

Restormer: Efficient transformer for high-resolution image restoration. S W Zamir, A Arora, S Khan, M Hayat, F S Khan, M.-H Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022

Pre-trained image processing transformer. H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, W Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202112310

All-in-one image restoration for unknown corruption. B Li, X Liu, P Hu, Z Wu, J Lv, X Peng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202217462

Relationship quantification of image degradations. W Wang, B Li, Y Gou, P Hu, X Peng, arXiv:2212.041482022arXiv preprint

Unsupervised degradation representation learning for blind super-resolution. L Wang, Y Wang, X Dong, Q Xu, J Yang, W An, Y Guo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202110590

Joint learning content and degradation aware feature for blind superresolution. Y Zhou, C Lin, D Luo, Y Liu, Y Tai, C Wang, M Chen, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022

Deep boosting for image denoising. C Chen, Z Xiong, X Tian, F Wu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018

Joint bi-layer optimization for single-image rain streak removal. L Zhu, C.-W Fu, D Lischinski, P.-A Heng, Proceedings. null2017

Recurrent squeezeand-excitation context aggregation net for single image deraining. X Li, J Wu, Z Lin, H Liu, H Zha, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018

Removing rain from single images via a deep detail network. X Fu, J Huang, D Zeng, Y Huang, X Ding, J Paisley, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017

A model-driven deep neural network for single image rain removal. H Wang, Q Xie, Q Zhao, D Meng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020

Joint rain detection and removal from a single image with contextualized deep networks. W Yang, R T Tan, J Feng, Z Guo, S Yan, J Liu, IEEE transactions on pattern analysis and machine intelligence. 201942

Image de-raining transformer. J Xiao, X Fu, A Liu, F Wu, Z.-J Zha, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022

Single image haze removal using dark channel prior. K He, J Sun, X Tang, IEEE transactions on pattern analysis and machine intelligence. 201033

You only look yourself: Unsupervised and untrained single image dehazing neural network. B Li, Y Gou, S Gu, J Z Liu, J T Zhou, X Peng, International Journal of Computer Vision. 1292021

Learning aggregated transmission propagation networks for haze removal and beyond. R Liu, X Fan, M Hou, Z Jiang, Z Luo, L Zhang, IEEE transactions on neural networks and learning systems. 201830

Residual dense network for image super-resolution. Y Zhang, Y Tian, Y Kong, B Zhong, Y Fu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018

Esrgan: Enhanced super-resolution generative adversarial networks. X Wang, K Yu, S Wu, J Gu, Y Liu, C Dong, Y Qiao, C Change Loy, Proceedings of the European conference on computer vision (ECCV) workshops. the European conference on computer vision (ECCV) workshops2018

Activating more pixels in image super-resolution transformer. X Chen, X Wang, J Zhou, Y Qiao, C Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202322377

Towards bidirectional arbitrary image rescaling: Joint optimization and cycle idempotence. Z Pan, B Li, D He, M Yao, W Wu, T Lin, X Li, E Ding, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202217398

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, ≈Å Kaiser, I Polosukhin, Advances in neural information processing systems. 201730

Noise2void-learning denoising from single noisy images. A Krull, T.-O Buchholz, F Jug, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019

Mprnet: Multi-path residual network for lightweight image super resolution. A Mehri, P B Ardakani, A D Sappa, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2021

Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020

All-in-one image restoration for unknown degradations using adaptive discriminative filters for specific degradations. D Park, B H Lee, S Y Chun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023

Ingredient-oriented multi-degradation learning for image restoration. J Zhang, J Huang, M Yao, Z Yang, H Yu, M Zhou, F Zhao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023

All in one bad weather removal using architectural search. R Li, R T Tan, L.-F Cheong, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020

Transweather: Transformer-based restoration of images degraded by adverse weather conditions. J M J Valanarasu, R Yasarla, V M Patel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022

Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model. W.-T Chen, Z.-K Huang, C.-C Tsai, H.-H Yang, J.-J Ding, S.-Y Kuo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202217662

Deep convolutional dictionary learning for image denoising. H Zheng, H Yong, L Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021

Blind image deblurring using dark channel prior. J Pan, D Sun, H Pfister, M.-H Yang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016

Image deblurring via enhanced low-rank prior. W Ren, X Cao, J Pan, X Guo, W Zuo, M.-H Yang, IEEE Transactions on Image Processing. 2572016

Tape: Task-agnostic prior embedding for image restoration. L Liu, L Xie, X Zhang, S Yuan, X Chen, W Zhou, H Li, Q Tian, European Conference on Computer Vision. Springer2022

Unfolding the alternating optimization for blind super resolution. Z Luo, Y Huang, S Li, L Wang, T Tan, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing Systems2020

Tensor decompositions and applications. T G Kolda, B W Bader, SIAM review. 5132009

Many-to-many splatting for efficient video frame interpolation. P Hu, S Niklaus, S Sclaroff, K Saenko, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022

The ubiquitous kronecker product. C F Van Loan, Journal of computational and applied mathematics. 1231-22000

Efficient compact bilinear pooling via kronecker product. T Yu, Y Cai, P Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236

A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. D Martin, C Fowlkes, D Tal, J Malik, Proceedings Eighth IEEE International Conference on Computer Vision. ICCV. Eighth IEEE International Conference on Computer Vision. ICCVIEEE2001. 20012

Single image super-resolution from transformed self-exemplars. J.-B Huang, A Singh, N Ahuja, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015

Multi-stage progressive image restoration. S W Zamir, A Arora, S Khan, M Hayat, F S Khan, M.-H Yang, L Shao, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202114831

Drt: A lightweight single image deraining recursive transformer. Y Liang, S Anwar, Y Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022

Benchmarking single-image dehazing and beyond. B Li, W Ren, D Fu, D Tao, D Feng, W Zeng, Z Wang, IEEE Transactions on Image Processing. 2812018

Vision transformers for single image dehazing. Y Song, Z He, H Qian, X Du, IEEE Transactions on Image Processing. 322023

Ntire 2017 challenge on single image super-resolution: Dataset and study. E Agustsson, R Timofte, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. July 2017

Image super-resolution with cross-scale non-local attention and exhaustive selfexemplars mining. Y Mei, Y Fan, Y Zhou, L Huang, T S Huang, H Shi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020

Waterloo exploration database: New challenges for image quality assessment models. K Ma, Z Duanmu, Q Wu, Z Wang, H Yong, H Li, L Zhang, IEEE Transactions on Image Processing. 2622016

Swinir: Image restoration using swin transformer. J Liang, J Cao, G Sun, K Zhang, L Van Gool, R Timofte, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021

Simple baselines for image restoration. L Chen, X Chu, X Zhang, J Sun, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part VII

Blind image quality assessment using a deep bilinear convolutional neural network. W Zhang, K Ma, J Yan, D Deng, Z Wang, IEEE Transactions on Circuits and Systems for Video Technology. 201830

Musiq: Multiscale image quality transformer. J Ke, Q Wang, Y Wang, P Milanfar, F Yang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021

A high-quality denoising dataset for smartphone cameras. A Abdelhamed, S Lin, M S Brown, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018

Deep joint rain detection and removal from a single image. W Yang, R T Tan, J Feng, J Liu, Z Guo, S Yan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017

Benchmarking single-image dehazing and beyond. B Li, W Ren, D Fu, D Tao, D Feng, W Zeng, Z Wang, IEEE Transactions on Image Processing. 2812019
